from rest_framework import parsers, status
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework.generics import ListCreateAPIView, RetrieveUpdateDestroyAPIView
from rest_framework.permissions import AllowAny
from django.conf import settings
from django.http import HttpResponse
from django.core.files.base import ContentFile
from django.core.files.storage import default_storage
import json
from openai import OpenAI
import re
from weasyprint import HTML, CSS
from datetime import datetime
import os
import requests
from urllib.parse import urlparse
import hashlib
from bs4 import BeautifulSoup
import pathlib
from pathlib import Path

# Chemin vers le r√©pertoire du projet
BASE_DIR = Path(__file__).resolve().parent.parent

# Recherche web en temps r√©el (optionnel - n√©cessite TAVILY_API_KEY)
try:
    from tavily import TavilyClient
    TAVILY_AVAILABLE = True
except ImportError:
    TAVILY_AVAILABLE = False
    print("‚ö†Ô∏è Tavily non disponible. Installer avec: pip install tavily-python")

# Scraping JavaScript (optionnel - n√©cessite playwright)
try:
    from playwright.sync_api import sync_playwright
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False
    print("‚ö†Ô∏è Playwright non disponible. Installer avec: pip install playwright && playwright install chromium")
import base64
import io
import fitz  # PyMuPDF
import traceback
from .models import Document, DocumentAsset, Folder

client = OpenAI(api_key=settings.OPENAI_API_KEY)

# Imports pour OpenAI


class APIRootView(APIView):
    """
    Vue racine de l'API - Liste tous les endpoints disponibles
    """
    permission_classes = [AllowAny]
    
    def get(self, request):
        return Response({
            "message": "Bienvenue sur l'API Invitation au Voyage",
            "version": "1.0.0",
            "status": "online",
            "endpoints": {
                "authentification": {
                    "register": "/api/auth/register/",
                    "login": "/api/auth/login/",
                    "logout": "/api/auth/logout/",
                    "profile": "/api/auth/profile/",
                    "check": "/api/auth/check/",
                    "password_reset": "/api/auth/password-reset/",
                    "password_reset_confirm": "/api/auth/password-reset-confirm/",
                },
                "offres": {
                    "generate_offer": "/api/generate-offer/",
                    "generate_pdf_offer": "/api/generate-pdf-offer/",
                    "improve_offer": "/api/improve-offer/",
                    "grapesjs_pdf_generator": "/api/grapesjs-pdf-generator/",
                    "pdf_to_gjs": "/api/pdf-to-gjs/",
                },
                "documents": {
                    "list_create": "/api/documents/",
                    "without_folder": "/api/documents/without-folder/",
                    "detail": "/api/documents/{id}/",
                    "generate_pdf": "/api/documents/{id}/generate-pdf/",
                    "move_to_folder": "/api/documents/{id}/move-to-folder/",
                },
                "folders": {
                    "list_create": "/api/folders/",
                    "detail": "/api/folders/{id}/",
                    "documents": "/api/folders/{id}/documents/",
                },
                "admin": "/admin/"
            },
            "documentation": "https://github.com/QuentiinRoland/invitationAuVoyage-backend"
        })

# Configuration des APIs d'images
UNSPLASH_KEY = os.getenv("UNSPLASH_ACCESS_KEY")
BING_KEY = os.getenv("BING_IMAGE_SUBSCRIPTION_KEY")

# Configuration du cache d'images
MEDIA_DIR = pathlib.Path("/tmp/offer_images")
MEDIA_DIR.mkdir(exist_ok=True, parents=True)

def search_unsplash(query: str, per_page: int = 1):
    """Recherche d'images via l'API Unsplash"""
    if not UNSPLASH_KEY:
        return []
    
    url = "https://api.unsplash.com/search/photos"
    try:
        r = requests.get(url, params={
            "query": query, 
            "per_page": per_page, 
            "orientation": "landscape"
        }, headers={
            "Accept-Version": "v1", 
            "Authorization": f"Client-ID {UNSPLASH_KEY}"
        }, timeout=3)  # R√©duit √† 3 secondes pour √©viter les timeouts
        r.raise_for_status()
        data = r.json().get("results", [])
        out = []
        for item in data:
            out.append({
                "url": item["urls"]["regular"],
                "thumb": item["urls"]["small"],
                "source": item["links"]["html"],
                "photographer": item["user"]["name"],
                "attribution": f'Photo: {item["user"]["name"]} / Unsplash',
                "license": "Unsplash License"
            })
        return out
    except Exception as e:
        print(f"Erreur Unsplash: {e}")
        return []

def search_bing_images(query: str, count: int = 1):
    """Recherche d'images via l'API Bing Image Search"""
    if not BING_KEY:
        return []
    
    url = "https://api.bing.microsoft.com/v7.0/images/search"
    try:
        r = requests.get(url, params={
            "q": query, 
            "count": count, 
            "safeSearch": "Moderate"
        }, headers={
            "Ocp-Apim-Subscription-Key": BING_KEY
        }, timeout=3)  # R√©duit √† 3 secondes pour √©viter les timeouts
        r.raise_for_status()
        data = r.json().get("value", [])
        out = []
        for item in data:
            out.append({
                "url": item.get("contentUrl"),
                "thumb": item.get("thumbnailUrl"),
                "source": item.get("hostPageUrl"),
                "attribution": item.get("hostPageDomainFriendlyName") or item.get("hostPageDisplayUrl"),
                "license": item.get("license", "Check site")
            })
        return out
    except Exception as e:
        print(f"Erreur Bing Images: {e}")
        return []

def cache_image(url: str) -> str:
    """Cache une image localement et retourne le chemin absolu pour WeasyPrint"""
    try:
        h = hashlib.sha1(url.encode()).hexdigest()
        ext = ".png" if url.lower().endswith('.png') else ".jpg"
        local = MEDIA_DIR / f"{h}{ext}"
        
        if not local.exists():
            print(f"üì• T√©l√©chargement image: {url}")
            r = requests.get(url, timeout=10)
            r.raise_for_status()
            local.write_bytes(r.content)
            print(f"‚úÖ Image t√©l√©charg√©e: {local} ({len(r.content)} bytes)")
        
        # Retourne le chemin absolu avec file:// pour WeasyPrint
        return f"file://{local.absolute()}"
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur cache image {url}: {e}")
        return url  # Retourne l'URL originale en cas d'erreur


class GrapesJSPDFGenerator(APIView):
    """
    Prend le contenu de GrapesJS et g√©n√®re un PDF (Chromium headless via Playwright)
    POST JSON: { html: string, css: string, company_info: {name, phone, email} }
    """
    permission_classes = [AllowAny]  # Acc√®s libre temporaire

    def post(self, request):
        html_content = request.data.get("html", "")
        css_content  = request.data.get("css", "")
        company_info = request.data.get("company_info", {}) or {}

        if not html_content:
            return Response({"error": "Contenu HTML requis"}, status=status.HTTP_400_BAD_REQUEST)

        try:
            # IMPORTANT: on recompose un HTML complet imprimable
            printable_html = self.convert_grapesjs_to_printable_html(html_content, css_content, company_info)

            # G√©n√©ration PDF avec WeasyPrint (solution standard Django pour production)
            # Support complet HTML5/CSS3, rendu professionnel, rapide et stable
            pdf_bytes = HTML(string=printable_html).write_pdf()

            resp = HttpResponse(pdf_bytes, content_type="application/pdf")
            resp["Content-Disposition"] = 'inline; filename="grapesjs-content.pdf"'
            return resp

        except Exception as e:
            # Log serveur utile pour debug
            import traceback; traceback.print_exc()
            return Response({"error": f"Erreur: {e}"}, status=500)

    # ---------------------------
    # M√©thodes utilitaires
    # ---------------------------

    def convert_grapesjs_to_printable_html(self, grapesjs_html: str, grapesjs_css: str, company_info: dict) -> str:
        """
        Convertit le contenu GrapesJS en HTML imprimable optimis√© pour WeasyPrint.
        """
        company_name  = company_info.get('name', 'Votre Entreprise')
        clean_html    = self.clean_grapesjs_html(grapesjs_html)
        clean_css     = self.clean_grapesjs_css(grapesjs_css)

        # Page HTML compl√®te avec le CSS embarqu√© optimis√© pour WeasyPrint
        return f"""<!DOCTYPE html>
<html lang="fr">
<head>
  <meta charset="utf-8">
  <title>Document - {company_name}</title>
  <style>
    /* Reset & base */
    * {{ margin: 0; padding: 0; box-sizing: border-box; }}
    
    /* Configuration de la page pour WeasyPrint - MULTI-PAGES avec background */
    @page {{
      size: A4;
      margin: 0;
    }}
    
    body {{
      font-family: Arial, Helvetica, sans-serif;
      line-height: 1.3;
      color: #333;
      background: transparent;
      width: 100%;
      max-width: 100%;
      padding: 0;
      margin: 0;
      font-size: 9pt;
      position: relative;
    }}
    
    /* Supprimer tous les espaces au-dessus du header */
    body::before {{
      content: none;
      display: none;
    }}

    /* CSS GrapesJS nettoy√© - Le background sera sur TOUTES les pages */
    {clean_css}

    /* Conteneur principal - padding pour descendre le texte sur TOUTES les pages */
    .grapesjs-content {{
      width: 100%;
      overflow: visible;
      position: relative;
      padding: 1cm 2cm 2cm 2cm;
    }}

    /* Sections compactes */
    .flight-section, .hotel-section, .price-section {{
      margin-bottom: 8px;
      padding: 8px;
      border-radius: 3px;
    }}
    
    .cta-section {{
      margin: 8px 0;
      padding: 10px;
      border-radius: 3px;
    }}
    
    .offer-header {{
      margin-bottom: 10px;
      padding: 10px;
      border-radius: 3px;
    }}
    
    /* Titres compacts */
    h1, h2, h3 {{ 
      page-break-after: avoid;
      page-break-inside: avoid;
      margin-bottom: 6px;
      margin-top: 8px;
      clear: both;
    }}
    
    h1 {{ 
      font-size: 16pt;
      margin-top: 0;
      line-height: 1.2;
    }}
    h2 {{ 
      font-size: 12pt;
      line-height: 1.2;
    }}
    h3 {{ 
      font-size: 10pt;
      line-height: 1.2;
    }}
    
    /* Paragraphes compacts */
    p {{
      margin-bottom: 5px;
      line-height: 1.3;
      orphans: 2;
      widows: 2;
    }}
    
    /* Images - compatible WeasyPrint */
    img {{
      max-width: 100%;
      height: auto;
      display: block;
      page-break-inside: avoid;
    }}
    
    /* Emp√™cher les d√©bordements */
    * {{
      max-width: 100%;
      word-wrap: break-word;
      overflow-wrap: break-word;
    }}
    
    /* Listes compactes */
    ul, ol {{
      margin-bottom: 5px;
      padding-left: 20px;
    }}
    
    li {{
      margin-bottom: 2px;
      line-height: 1.3;
    }}
    
    /* Footer compact */
    .footer {{
      margin-top: 15px;
      padding-top: 8px;
      font-size: 7pt;
    }}
    
  </style>
</head>
<body>
  <div class="grapesjs-content">
    {clean_html}
  </div>
</body>
</html>"""

    def clean_grapesjs_html(self, html: str) -> str:
        """Nettoie le HTML de GrapesJS pour l'impression avec WeasyPrint."""
        if not html:
            return ""
        
        # Supprimer les attributs GrapesJS
        html = re.sub(r'data-gjs-[^=]*="[^"]*"', '', html)
        html = re.sub(r'contenteditable="[^"]*"', '', html)
        html = re.sub(r'spellcheck="[^"]*"', '', html)
        html = re.sub(r'draggable="[^"]*"', '', html)
        
        # FORCER background-repeat: no-repeat dans les styles inline
        # Supprimer d'abord les background-repeat existants dans le style inline
        html = re.sub(
            r'style="([^"]*?)background-repeat:\s*[^;]+;?([^"]*)"',
            r'style="\1\2"',
            html,
            flags=re.IGNORECASE
        )
        
        # Ajouter background-repeat: no-repeat apr√®s chaque background dans style inline
        html = re.sub(
            r'style="([^"]*?)(background[^:]*:\s*[^";]+url\([^)]+\)[^";]*)',
            r'style="\1\2; background-repeat: no-repeat',
            html,
            flags=re.IGNORECASE
        )
        
        # Ajouter des styles inline pour les images si elles n'en ont pas
        html = re.sub(
            r'<img([^>]*?)(?:style="[^"]*")?([^>]*?)>',
            lambda m: f'<img{m.group(1)} style="max-width: 100%; height: auto; display: block;"{m.group(2)}>',
            html
        )
        
        # Supprimer les styles inline qui causent des probl√®mes avec WeasyPrint
        html = re.sub(r'position:\s*absolute\s*;?', '', html, flags=re.IGNORECASE)
        html = re.sub(r'position:\s*fixed\s*;?', '', html, flags=re.IGNORECASE)
        html = re.sub(r'transform:[^;]+;?', '', html, flags=re.IGNORECASE)
        
        # Nettoyer les espaces multiples
        html = re.sub(r'\s+', ' ', html).strip()
        html = re.sub(r'>\s+<', '><', html)
        
        return html

    def clean_grapesjs_css(self, css: str) -> str:
        """Nettoie le CSS de GrapesJS pour WeasyPrint."""
        if not css:
            return ""
        
        # Supprimer les attributs GrapesJS
        css = re.sub(r'\[data-gjs[^\]]*\][^}]*}', '', css)
        css = re.sub(r'\.gjs-[^}]*}', '', css)
        
        # Remplacer les valeurs transparentes
        css = css.replace('rgba(0,0,0,0)', 'transparent')
        
        # FORCER background-repeat: no-repeat sur TOUT ce qui a un background
        # Supprimer d'abord les background-repeat existants
        css = re.sub(r'background-repeat:\s*[^;]+;', '', css, flags=re.IGNORECASE)
        
        # Ajouter background-repeat: no-repeat apr√®s chaque background-image
        css = re.sub(
            r'(background-image:\s*url\([^)]+\))',
            r'\1; background-repeat: no-repeat',
            css,
            flags=re.IGNORECASE
        )
        
        # Ajouter background-repeat: no-repeat apr√®s chaque background:
        css = re.sub(
            r'(background:\s*[^;]+;)',
            lambda m: m.group(1).rstrip(';') + '; background-repeat: no-repeat;' if 'url(' in m.group(1) else m.group(1),
            css,
            flags=re.IGNORECASE
        )
        
        # Supprimer les propri√©t√©s CSS non support√©es par WeasyPrint
        unsupported_properties = [
            r'transform:[^;]+;',
            r'animation:[^;]+;',
            r'transition:[^;]+;',
            r'box-shadow:[^;]+;',
            r'text-shadow:[^;]+;',
            r'filter:[^;]+;',
            r'backdrop-filter:[^;]+;',
            r'clip-path:[^;]+;',
            r'mask:[^;]+;',
            r'cursor:[^;]+;',
            r'pointer-events:[^;]+;',
            r'user-select:[^;]+;',
            r'-webkit-[^:]+:[^;]+;',
            r'-moz-[^:]+:[^;]+;',
            r'-ms-[^:]+:[^;]+;',
        ]
        
        for prop in unsupported_properties:
            css = re.sub(prop, '', css, flags=re.IGNORECASE)
        
        # Convertir les positionnements absolus/fixed en relatif pour √©viter les d√©bordements
        css = re.sub(r'position:\s*absolute\s*;', 'position: relative;', css, flags=re.IGNORECASE)
        css = re.sub(r'position:\s*fixed\s*;', 'position: relative;', css, flags=re.IGNORECASE)
        
        # Limiter les largeurs en pourcentage pour √©viter les d√©bordements
        css = re.sub(r'width:\s*(\d+)vw\s*;', lambda m: f'width: {min(100, int(m.group(1)))}%;', css)
        
        # Nettoyer les espaces
        css = re.sub(r'\s+', ' ', css).strip()
        css = re.sub(r'\s*{\s*', ' { ', css)
        css = re.sub(r'\s*}\s*', ' } ', css)
        css = re.sub(r'\s*;\s*', '; ', css)
        css = re.sub(r';\s*}', ' }', css)
        
        return css

    def generate_footer(self, company_info: dict, current_date: str) -> str:
        """Footer simple avec infos entreprise + date."""
        company_name = company_info.get('name', '')
        company_phone = company_info.get('phone', '')
        company_email = company_info.get('email', '')

        if not (company_name or company_phone or company_email):
            return ""

        footer_content = []
        if company_name:
            footer_content.append(f"<strong>{company_name}</strong>")

        contact_parts = []
        if company_phone:
            contact_parts.append(company_phone)
        if company_email:
            contact_parts.append(company_email)
        if contact_parts:
            footer_content.append(" ‚Ä¢ ".join(contact_parts))

        footer_content.append(f"Document g√©n√©r√© le {current_date}")

        return f"""
  <div style="margin-top: 40px; padding: 20px; border-top: 2px solid #ddd; text-align: center; color: #666; font-size: 12px;">
    {"<br>".join(footer_content)}
  </div>"""



class TravelOfferGenerator(APIView):
    permission_classes = [AllowAny]  # Acc√®s libre temporaire
    
    def _load_default_templates(self, offer_type):
        """
        Charge les templates par d√©faut depuis les fichiers JSON selon le type d'offre.
        """
        try:
            template_path = BASE_DIR / 'api' / 'templates' / f'{offer_type}_example.json'
            if template_path.exists():
                with open(template_path, 'r', encoding='utf-8') as f:
                    template_data = json.load(f)
                    return [template_data]  # Retourner sous forme de liste
            else:
                print(f"‚ö†Ô∏è Template par d√©faut non trouv√©: {template_path}")
                return None
        except Exception as e:
            print(f"‚ùå Erreur chargement template par d√©faut: {str(e)}")
            return None
    
    def _scrape_website_description(self, url, max_length=2000):
        """
        R√©cup√®re la description d'un site web en scrapant son contenu.
        Extrait le texte principal et les meta descriptions.
        ATTENTION: Cette m√©thode ne peut pas ex√©cuter JavaScript.
        Pour les sites JS/Angular/React, utiliser _scrape_with_tavily().
        """
        try:
            # S'assurer que l'URL a un protocole
            original_url = url
            if not url.startswith(('http://', 'https://')):
                url = 'https://' + url
                print(f"üîß URL corrig√©e: {original_url} ‚Üí {url}")
            
            # D√©tecter si l'URL contient beaucoup de param√®tres (souvent signe d'un site JS)
            if '?' in url and len(url.split('?')[1]) > 100:
                print(f"   ‚ö†Ô∏è URL avec beaucoup de param√®tres d√©tect√©e - ce site utilise probablement JavaScript")
                print(f"   üí° Recommandation: Ce type de site n√©cessite Tavily pour √™tre correctement scrap√©")
            
            print(f"üì° Requ√™te HTTP vers: {url}")
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Referer': 'https://www.google.com/'  # Simuler une arriv√©e depuis Google
            }
            print(f"‚è±Ô∏è Timeout de 20 secondes pour le scraping...")
            response = requests.get(url, headers=headers, timeout=20, allow_redirects=True, verify=False)
            print(f"üì• R√©ponse HTTP: {response.status_code}")
            
            if response.status_code != 200:
                print(f"‚ö†Ô∏è Code HTTP non-200: {response.status_code}")
                return None
            
            response.raise_for_status()
            
            # V√©rifier que le contenu n'est pas vide
            if not response.content or len(response.content) < 100:
                print(f"‚ö†Ô∏è R√©ponse vide ou trop courte: {len(response.content) if response.content else 0} bytes")
                return None
            
            print(f"üìÑ Taille du contenu HTML: {len(response.content)} bytes")
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Supprimer les scripts et styles
            for script in soup(["script", "style", "nav", "footer", "header"]):
                script.decompose()
            
            # Extraire meta description
            meta_desc = ""
            meta_tag = soup.find("meta", attrs={"name": "description"})
            if meta_tag and meta_tag.get("content"):
                meta_desc = meta_tag.get("content")
            
            # Extraire le texte principal
            text_content = soup.get_text(separator=' ', strip=True)
            
            # D√©tecter si le contenu contient beaucoup de templates Angular/React/JS non rendus
            js_templates = ['{{', 'ng-', '[ng', '*ng', 'v-if', 'v-for', '@click', 'className', 'useState']
            has_js_templates = any(template in str(response.content) for template in js_templates)
            
            if has_js_templates:
                print(f"   ‚ö†Ô∏è Site JavaScript d√©tect√© (templates non rendus trouv√©s)")
                print(f"   üí° Ce site n√©cessite JavaScript pour afficher le contenu. BeautifulSoup ne peut pas l'ex√©cuter.")
                print(f"   üí° Le contenu extrait sera probablement incomplet (templates {{ }})")
            
            # Nettoyer le texte (supprimer espaces multiples)
            lines = (line.strip() for line in text_content.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text_content = ' '.join(chunk for chunk in chunks if chunk)
            
            # Combiner meta description + texte principal
            full_content = ""
            if meta_desc:
                full_content = f"Description: {meta_desc}\n\n"
            
            # Limiter la longueur
            if len(text_content) > max_length:
                text_content = text_content[:max_length] + "..."
            
            full_content += text_content
            
            # V√©rifier que le contenu final n'est pas vide
            if not full_content or len(full_content.strip()) < 50:
                print(f"‚ö†Ô∏è Contenu extrait trop court: {len(full_content)} caract√®res")
                print(f"   Preview: {full_content[:200]}")
                return None
            
            print(f"‚úÖ Contenu extrait: {len(full_content)} caract√®res")
            print(f"   Preview: {full_content[:300]}...")
            
            return full_content
            
        except requests.exceptions.Timeout:
            print(f"‚è±Ô∏è Timeout lors du scraping de {url} (le site prend trop de temps √† r√©pondre)")
            return None
        except requests.exceptions.RequestException as e:
            print(f"‚ùå Erreur HTTP lors du scraping de {url}: {str(e)}")
            if hasattr(e, 'response') and e.response is not None:
                print(f"   Status code: {e.response.status_code}")
            return None
        except Exception as e:
            print(f"‚ùå Erreur scraping {url}: {type(e).__name__}: {str(e)}")
            import traceback
            print(f"   Traceback: {traceback.format_exc()}")
            return None
    
    def _get_website_descriptions(self, urls):
        """
        R√©cup√®re les descriptions de plusieurs sites web.
        Retourne une liste de descriptions avec statut pour le debugging.
        Pour les sites JavaScript/Angular/React, utilise Tavily directement.
        """
        descriptions = []
        failed_urls = []
        for url in urls:
            if url and url.strip():
                url_clean = url.strip()
                print(f"üåê Tentative de scraping: {url_clean}")
                
                # D√©tecter si c'est probablement un site JavaScript (misterfly, booking, etc.)
                js_sites_keywords = ['misterfly', 'booking.com', 'expedia', 'airbnb', 'vrbo', 'hotels.com']
                is_js_site = any(keyword in url_clean.lower() for keyword in js_sites_keywords)
                
                # Strat√©gie : Essayer Playwright (navigateur headless) en premier pour les sites JS
                # Si Playwright n'est pas disponible, utiliser Tavily, sinon BeautifulSoup
                desc = None
                
                if is_js_site and PLAYWRIGHT_AVAILABLE:
                    print(f"   üé≠ Site JavaScript d√©tect√©, utilisation de Playwright (navigateur headless)...")
                    desc = self._scrape_with_playwright(url_clean)
                
                # Si Playwright √©choue ou n'est pas disponible, essayer Tavily
                if (not desc or len(desc.strip()) < 50) and TAVILY_AVAILABLE:
                    if desc:
                        print(f"   ‚ö†Ô∏è Playwright √©chou√©, essai avec Tavily...")
                    else:
                        print(f"   üîç Tentative avec Tavily...")
                    desc_tavily = self._scrape_with_tavily(url_clean)
                    if desc_tavily:
                        desc = desc_tavily
                
                # Si tout √©choue, essayer le scraping classique (pour les sites HTML simples)
                if not desc or len(desc.strip()) < 50:
                    if desc:
                        print(f"   ‚ö†Ô∏è M√©thodes avanc√©es √©chou√©es, essai avec scraping classique...")
                    else:
                        print(f"   üìÑ Tentative avec scraping classique (BeautifulSoup)...")
                    desc = self._scrape_website_description(url_clean)
                
                if desc and len(desc.strip()) > 50:  # V√©rifier que le contenu n'est pas vide
                    # Extraire aussi les images
                    images = []
                    if is_js_site and PLAYWRIGHT_AVAILABLE:
                        images = self._extract_images_with_playwright(url_clean)
                    elif desc:  # Si on a du contenu, extraire les images de l'URL
                        images = self._extract_images_from_url(url_clean)
                    
                    descriptions.append({
                        "url": url_clean,
                        "content": desc,
                        "images": images[:5] if images else []  # Limiter √† 5 images max
                    })
                    if images:
                        print(f"‚úÖ Scraping r√©ussi pour: {url_clean} ({len(desc)} caract√®res, {len(images)} image(s) trouv√©e(s))")
                    else:
                        print(f"‚úÖ Scraping r√©ussi pour: {url_clean} ({len(desc)} caract√®res)")
                else:
                    failed_urls.append(url_clean)
                    print(f"‚ùå √âchec du scraping pour: {url_clean} (toutes les m√©thodes ont √©chou√©)")
                    if not TAVILY_AVAILABLE:
                        print(f"   üí° Astuce: Tavily n'est pas configur√©. Pour les sites JavaScript, il est recommand√© d'ajouter TAVILY_API_KEY dans .env")
        if failed_urls:
            print(f"‚ö†Ô∏è {len(failed_urls)} URL(s) n'ont pas pu √™tre scrapp√©es: {', '.join(failed_urls)}")
        return descriptions
    
    def _search_flights_with_airfrance_klm(self, origin_code, destination_code, travel_date, return_date=None, search_metadata=None):
        """
        Recherche de VRAIS vols avec l'API Air France-KLM.
        Retourne des vols r√©els avec horaires, num√©ros de vol, compagnies.
        Documentation: https://developer.airfranceklm.com/products/api/flightstatus/api-reference
        
        Args:
            origin_code: Code IATA de l'a√©roport de d√©part (ex: 'CDG')
            destination_code: Code IATA de l'a√©roport de destination (ex: 'DPS')
            travel_date: Date de voyage au format YYYY-MM-DD
            return_date: Date de retour au format YYYY-MM-DD (optionnel)
            search_metadata: Dict pour stocker les m√©tadonn√©es de recherche (d√©tails d'erreur API, etc.)
        """
        from django.conf import settings
        import time
        import random
        
        print("=" * 80)
        print("‚úàÔ∏è‚úàÔ∏è‚úàÔ∏è D√âBUT RECHERCHE AIR FRANCE-KLM API ‚úàÔ∏è‚úàÔ∏è‚úàÔ∏è")
        print("=" * 80)
        
        api_key = getattr(settings, 'AIRFRANCE_KLM_API_KEY', None)
        
        if not api_key:
            print("‚ùå‚ùå‚ùå ERREUR: AIRFRANCE_KLM_API_KEY non configur√©e")
            if search_metadata is not None:
                search_metadata['failure_reason'] = ['credentials_missing']
            return None
        
        # Fonction pour faire des requ√™tes avec backoff exponentiel
        def make_api_request(url, params, headers, max_retries=3):
            for attempt in range(max_retries):
                try:
                    # D√©lai exponentiel avec jitter
                    if attempt > 0:
                        backoff_time = (2 ** attempt) + random.uniform(0, 1)
                        print(f"‚è±Ô∏è Tentative {attempt+1}/{max_retries} - Attente de {backoff_time:.2f} secondes...")
                        time.sleep(backoff_time)
                    
                    print(f"üì° Requ√™te vers {url}")
                    print(f"   - Param√®tres: {params}")
                    print(f"   - Headers: {list(headers.keys())}")
                    
                    response = requests.get(url, params=params, headers=headers, timeout=30)
                    
                    print(f"üìä Code de statut: {response.status_code}")
                    if response.status_code == 200:
                        return response.json()
                    elif response.status_code == 403 and "Over Qps" in response.text:
                        print(f"‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è ERREUR: 'Developer Over Qps' - Trop de requ√™tes par seconde")
                    else:
                        print(f"‚ùå Erreur API: {response.status_code} - {response.text[:200]}")
                        if search_metadata is not None:
                            search_metadata['failure_reason'] = ['api_error']
                            search_metadata['api_error'] = {
                                'status_code': response.status_code,
                                'error_message': response.text[:200]
                            }
                        
                except Exception as e:
                    print(f"‚ùå Exception lors de la requ√™te: {str(e)}")
                    
                # Si nous arrivons ici, c'est que la requ√™te a √©chou√©
                if attempt == max_retries - 1:
                    return None
            
            return None
        
        print(f"üìã Param√®tres de recherche:")
        print(f"   - Origin: {origin_code}")
        print(f"   - Destination: {destination_code}")
        print(f"   - Date aller: {travel_date}")
        if return_date:
            print(f"   - Date retour: {return_date}")
        print()
        
        # PRIORIT√â ABSOLUE : API Offers uniquement (pas FlightStatus)
        # Documentation: https://developer.airfranceklm.com/products/api/offers/api-reference/paths/opendata-offers-v3-available-offers/post
        print("\nüîç RECHERCHE EXCLUSIVE AVEC API OFFERS v3/available-offers...")
        
        # L'endpoint correct est /opendata/offers/v3/available-offers (POST uniquement)
        offers_url = "https://api.airfranceklm.com/opendata/offers/v3/available-offers"
        
        # Construire le body selon la documentation
        # Structure: requestedConnections avec origin/destination en objets
        requested_connections = [
            {
                "departureDate": travel_date,
                "origin": {
                    "code": origin_code,
                    "type": "STOPOVER"
                },
                "destination": {
                    "code": destination_code,
                    "type": "STOPOVER"
                }
            }
        ]
        
        # Si date retour, ajouter la connection retour
        if return_date:
            requested_connections.append({
                "departureDate": return_date,
                "origin": {
                    "code": destination_code,
                    "type": "STOPOVER"
                },
                "destination": {
                    "code": origin_code,
                    "type": "STOPOVER"
                }
            })
        
        # Body selon la documentation
        offers_body = {
            "commercialCabins": ["ALL"],
            "bookingFlow": "LEISURE",
            "passengers": [
                {
                    "id": 1,
                    "type": "ADT"
                }
            ],
            "requestedConnections": requested_connections
        }
        
        # Headers selon la documentation (Content-Type: application/hal+json)
        offers_headers = {
            "API-Key": api_key,
            "AFKL-TRAVEL-Host": "KL",  # Ou "AF" pour Air France
            "Accept": "application/hal+json",
            "Content-Type": "application/hal+json"  # IMPORTANT: hal+json selon la doc
        }
        
        print(f"\n   üîç POST vers {offers_url}")
        print(f"   üìã Body: {json.dumps(offers_body, indent=2)}")
        
        offers_response = None
        try:
            # Utiliser data= avec json.dumps pour avoir application/hal+json
            response = requests.post(
                offers_url,
                data=json.dumps(offers_body),  # Envoyer comme string JSON
                headers=offers_headers,
                timeout=30
            )
            print(f"   üìä Code de statut: {response.status_code}")
            
            if response.status_code == 200:
                offers_response = response.json()
                print(f"   ‚úÖ R√©ponse re√ßue avec succ√®s!")
            elif response.status_code == 404:
                print(f"   ‚ùå Endpoint 404 - v√©rifiez que l'endpoint est correct")
                print(f"   üìã R√©ponse: {response.text[:500]}")
            elif response.status_code == 403:
                print(f"   ‚ö†Ô∏è Erreur 403 - v√©rifiez vos credentials")
                print(f"   üìã R√©ponse: {response.text[:500]}")
            else:
                print(f"   ‚ùå Erreur {response.status_code}: {response.text[:500]}")
        except Exception as e:
            print(f"   ‚ùå Exception: {type(e).__name__}: {str(e)[:200]}")
        
        if offers_response:
            print("‚úÖ R√©ponse de l'API Offers v3 re√ßue!")
            print(f"   üìã Structure r√©ponse: {list(offers_response.keys())[:10]}")
            
            # Traiter la r√©ponse HAL+JSON selon la documentation
            # La r√©ponse contient: recommendations, flightProducts, connections, etc.
            flights = []
            
            # Chercher dans recommendations (premi√®re option)
            recommendations = offers_response.get('recommendations', [])
            if recommendations:
                print(f"üìã {len(recommendations)} recommandation(s) trouv√©e(s)")
                for rec_idx, rec in enumerate(recommendations[:3], 1):  # Prendre les 3 premiers
                    print(f"   üîç Traitement recommandation {rec_idx}...")
                    
                    # Les flightProducts contiennent les connections
                    flight_products = rec.get('flightProducts', [])
                    if flight_products:
                        print(f"      üìã {len(flight_products)} flightProduct(s) dans cette recommandation")
                        for product_idx, product in enumerate(flight_products[:2], 1):  # Prendre les 2 premiers products
                            print(f"         üîç Traitement flightProduct {product_idx}...")
                            
                            # Les connections sont dans le flightProduct
                            connections = product.get('connections', [])
                            if connections:
                                print(f"            üìã {len(connections)} connection(s) dans ce product")
                                # connections est un tableau de tableaux (une connection par direction)
                                for conn_group_idx, conn_group in enumerate(connections):
                                    if isinstance(conn_group, list):
                                        for conn_idx, connection in enumerate(conn_group[:1], 1):
                                            print(f"               üîç Extraction connection {conn_group_idx}-{conn_idx}...")
                                            flight_info = self._extract_flight_info_from_offer_response(connection, origin_code, destination_code)
                                            if flight_info:
                                                print(f"                  ‚úÖ Vol extrait: {flight_info['flight_number']}")
                                                flights.append(flight_info)
                                    elif isinstance(conn_group, dict):
                                        print(f"               üîç Extraction connection {conn_group_idx}...")
                                        flight_info = self._extract_flight_info_from_offer_response(conn_group, origin_code, destination_code)
                                        if flight_info:
                                            print(f"                  ‚úÖ Vol extrait: {flight_info['flight_number']}")
                                            flights.append(flight_info)
                    
                    # Si pas de flightProducts, essayer directement les connections dans la recommendation
                    if not flights:
                        connections = rec.get('connections', [])
                        if connections:
                            print(f"      üìã {len(connections)} connection(s) directe(s) dans cette recommandation")
                            for conn_group in connections:
                                if isinstance(conn_group, list):
                                    for connection in conn_group[:1]:
                                        flight_info = self._extract_flight_info_from_offer_response(connection, origin_code, destination_code)
                                        if flight_info:
                                            flights.append(flight_info)
                                elif isinstance(conn_group, dict):
                                    flight_info = self._extract_flight_info_from_offer_response(conn_group, origin_code, destination_code)
                                    if flight_info:
                                        flights.append(flight_info)
            
            # Si pas de recommendations, chercher dans flightProducts directement
            if not flights:
                flight_products = offers_response.get('flightProducts', [])
                if flight_products:
                    print(f"üìã {len(flight_products)} flightProduct(s) trouv√©(s)")
                    for product in flight_products[:3]:
                        flight_info = self._extract_flight_info_from_offer_response(product, origin_code, destination_code)
                        if flight_info:
                            flights.append(flight_info)
            
            # Fallback: chercher dans connections directement (au niveau sup√©rieur de la r√©ponse)
            if not flights:
                connections = offers_response.get('connections', [])
                if connections:
                    print(f"üìã {len(connections)} connection(s) trouv√©e(s) au niveau sup√©rieur")
                    # Afficher la structure compl√®te d'une connection pour debug
                    if connections and len(connections) > 0:
                        first_conn = connections[0]
                        if isinstance(first_conn, list) and len(first_conn) > 0:
                            first_conn = first_conn[0]
                        print(f"   üìã Structure premi√®re connection (compl√®te): {json.dumps(first_conn, indent=2)[:1500]}")
                    
                    for conn_idx, conn in enumerate(connections[:2], 1):  # Prendre les 2 premi√®res
                        if isinstance(conn, list):
                            for c_idx, c in enumerate(conn[:2], 1):
                                print(f"   üîç Extraction connection top-level {conn_idx}-{c_idx}...")
                                flight_info = self._extract_flight_info_from_offer_response(c, origin_code, destination_code)
                                if flight_info:
                                    flights.append(flight_info)
                        else:
                            print(f"   üîç Extraction connection top-level {conn_idx}...")
                            flight_info = self._extract_flight_info_from_offer_response(conn, origin_code, destination_code)
                            if flight_info:
                                flights.append(flight_info)
            
            # Si toujours pas de vols, chercher dans _links (HAL+JSON peut avoir des liens vers les d√©tails)
            if not flights and offers_response.get('_links'):
                print(f"üìã _links trouv√© - les d√©tails des vols peuvent √™tre dans les liens HAL+JSON")
                links = offers_response['_links']
                print(f"   üìã Cl√©s dans _links: {list(links.keys())[:10]}")
                # Note: Suivre les liens n√©cessiterait des requ√™tes suppl√©mentaires
                # Pour l'instant on ne le fait pas, mais on peut logger les URLs disponibles
            
            if flights:
                print(f"‚úÖ {len(flights)} vol(s) extrait(s) depuis l'API Offers")
                return flights
            else:
                print(f"‚ö†Ô∏è Aucun vol extrait - structure de r√©ponse inattendue")
                # Sauvegarder la r√©ponse compl√®te dans un fichier pour debug
                try:
                    debug_file = '/tmp/airfrance_klm_response_debug.json'
                    with open(debug_file, 'w', encoding='utf-8') as f:
                        json.dump(offers_response, f, indent=2, ensure_ascii=False)
                    print(f"   üí° R√©ponse compl√®te sauvegard√©e dans {debug_file} pour analyse")
                except Exception as e:
                    print(f"   ‚ö†Ô∏è Impossible de sauvegarder: {str(e)}")
                print(f"   üìã R√©ponse compl√®te (premiers 3000 chars): {json.dumps(offers_response, indent=2)[:3000]}")
        
        # Si toutes les tentatives Offers √©chouent
        print("\n‚ùå Aucun vol trouv√© avec l'API Offers")
        if offers_response:
            print("‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è L'API a retourn√© une r√©ponse mais aucun vol n'a pu √™tre extrait")
            print("üí° La structure de la r√©ponse peut √™tre diff√©rente de celle attendue")
            print("üí° V√©rifiez les logs ci-dessus pour voir la structure exacte de la r√©ponse")
        else:
            print("‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è L'API Offers n'a pas retourn√© de r√©ponse valide")
            print("üí° V√©rifiez vos credentials et que l'endpoint est correct")
        
        # NE PLUS UTILISER FlightStatus - uniquement Offers
        if search_metadata is not None:
            if offers_response:
                search_metadata['failure_reason'] = ['offers_api_responded_but_no_flights_extracted']
            else:
                search_metadata['failure_reason'] = ['offers_api_failed', 'no_response']
        print("=" * 80)
        return None
        
    def _extract_flight_info_from_offer_response(self, connection_or_product, origin_code, destination_code):
        """
        Extrait les informations de vol depuis une connection ou flightProduct de la r√©ponse Offers API.
        Structure attendue (d'apr√®s les logs) :
        - segments[].marketingFlight.number
        - segments[].departureDateTime
        - segments[].arrivalDateTime
        - segments[].origin.code
        - segments[].destination.code
        - segments[].marketingFlight.carrier.name/code
        """
        try:
            # Structure HAL+JSON peut varier, essayer plusieurs formats
            flight_number = None
            airline_name = "Air France-KLM"
            departure_airport = origin_code
            departure_time = None
            arrival_airport = destination_code
            arrival_time = None
            
            # Extraire depuis segments (structure Offers API avec marketingFlight)
            segments = connection_or_product.get('flightSegments', []) or connection_or_product.get('segments', [])
            if segments:
                # Prendre le premier segment pour d√©part et dernier pour arriv√©e
                first_segment = segments[0] if segments else {}
                last_segment = segments[-1] if segments else first_segment
                
                # IMPORTANT: Structure Offers API utilise marketingFlight.number (pas flightNumber)
                # Format: marketingFlight: {number: "2016", carrier: {code: "KL", name: "KLM"}}
                marketing_flight = first_segment.get('marketingFlight', {})
                if isinstance(marketing_flight, dict):
                    flight_num = marketing_flight.get('number')
                    carrier = marketing_flight.get('carrier', {})
                    carrier_code = None
                    if isinstance(carrier, dict):
                        airline_name = carrier.get('name', carrier.get('code', 'Air France-KLM'))
                        carrier_code = carrier.get('code')
                    
                    # Formater le num√©ro de vol avec le code compagnie (ex: KL2016)
                    if flight_num and carrier_code:
                        flight_number = f"{carrier_code}{flight_num}"
                    elif flight_num:
                        flight_number = str(flight_num)
                    
                    # Si pas de flightNumber, essayer operatingFlight
                    if not flight_number:
                        operating_flight = first_segment.get('operatingFlight', {})
                        if isinstance(operating_flight, dict):
                            flight_num = operating_flight.get('number')
                            op_carrier = operating_flight.get('carrier', {})
                            if isinstance(op_carrier, dict):
                                op_carrier_code = op_carrier.get('code')
                                if flight_num and op_carrier_code:
                                    flight_number = f"{op_carrier_code}{flight_num}"
                                elif flight_num:
                                    flight_number = str(flight_num)
                                if not airline_name or airline_name == 'Air France-KLM':
                                    airline_name = op_carrier.get('name', op_carrier.get('code', 'Air France-KLM'))
                
                # Si toujours pas de flightNumber, essayer les autres formats
                if not flight_number:
                    flight_number = (
                        first_segment.get('flightNumber') or 
                        first_segment.get('number') or 
                        first_segment.get('flight', {}).get('number') if isinstance(first_segment.get('flight'), dict) else None
                    )
                
                # D√©part depuis le premier segment
                # Format Offers API: origin: {code: "CDG"}, departureDateTime: "2025-11-23T16:55:00"
                origin_info = first_segment.get('origin', {})
                if isinstance(origin_info, dict):
                    departure_airport = origin_info.get('code') or origin_info.get('iata') or origin_code
                
                # D√©part time depuis departureDateTime
                departure_time = first_segment.get('departureDateTime') or first_segment.get('departure') or first_segment.get('departureInformation', {}).get('datetime')
                if not departure_time and isinstance(first_segment.get('departure'), dict):
                    departure_time = first_segment['departure'].get('datetime') or first_segment['departure'].get('time') or first_segment['departure'].get('scheduled')
                
                # Arriv√©e depuis le dernier segment
                destination_info = last_segment.get('destination', {})
                if isinstance(destination_info, dict):
                    arrival_airport = destination_info.get('code') or destination_info.get('iata') or destination_code
                
                # Arriv√©e time depuis arrivalDateTime
                arrival_time = last_segment.get('arrivalDateTime') or last_segment.get('arrival') or last_segment.get('arrivalInformation', {}).get('datetime')
                if not arrival_time and isinstance(last_segment.get('arrival'), dict):
                    arrival_time = last_segment['arrival'].get('datetime') or last_segment['arrival'].get('time') or last_segment['arrival'].get('scheduled')
            
            # Format 2: champs directs dans connection/product (fallback)
            if not flight_number:
                flight_number = (
                    connection_or_product.get('flightNumber') or 
                    connection_or_product.get('number') or
                    connection_or_product.get('flight', {}).get('number') if isinstance(connection_or_product.get('flight'), dict) else None
                )
            
            # Compagnie depuis connection/product (fallback)
            if airline_name == 'Air France-KLM':
                airline_info = connection_or_product.get('airline') or connection_or_product.get('flight', {}).get('airline')
                if airline_info:
                    if isinstance(airline_info, dict):
                        airline_name = airline_info.get('name', 'Air France-KLM')
                    else:
                        airline_name = airline_info
            
            # Formater le temps si n√©cessaire
            if departure_time:
                dt = self._parse_iso_datetime(departure_time)
                departure_time = dt.strftime('%Y-%m-%d %H:%M') if dt else str(departure_time)
            else:
                departure_time = 'N/A'
            
            if arrival_time:
                dt = self._parse_iso_datetime(arrival_time)
                arrival_time = dt.strftime('%Y-%m-%d %H:%M') if dt else str(arrival_time)
            else:
                arrival_time = 'N/A'
            
            if flight_number:
                return {
                    'flight_number': str(flight_number),
                    'airline': airline_name,
                    'departure_airport': departure_airport,
                    'departure_time': departure_time,
                    'arrival_airport': arrival_airport,
                    'arrival_time': arrival_time
                }
            return None
        except Exception as e:
            print(f"   ‚ö†Ô∏è Erreur extraction info vol: {type(e).__name__}: {str(e)[:200]}")
            import traceback
            print(f"   üìã Traceback: {traceback.format_exc()[:500]}")
            return None
    
    def _parse_iso_datetime(self, datetime_str):
        """
        Parse une cha√Æne de date/heure ISO 8601 en objet datetime.
        G√®re les fuseaux horaires.
        """
        try:
            import dateutil.parser
            
            dt = dateutil.parser.parse(datetime_str)
            return dt
        except Exception as e:
            print(f"‚ùå Erreur lors du parsing de la date: {str(e)}")
            return None
    
    def _search_flights_with_aviationstack(self, origin_code, destination_code, travel_date, return_date=None):
        """
        Recherche de VRAIS vols avec Aviationstack API (FALLBACK).
        Retourne des vols r√©els avec horaires, num√©ros de vol, compagnies, prix.
        """
        from django.conf import settings
        
        api_key = getattr(settings, 'AVIATIONSTACK_API_KEY', None)
        if not api_key:
            print("‚ö†Ô∏è AVIATIONSTACK_API_KEY non configur√©e - fallback d√©sactiv√©")
            return None
        
        try:
            print(f"‚úàÔ∏è Recherche de VRAIS vols avec Aviationstack: {origin_code} ‚Üí {destination_code}")
            print(f"   Date aller: {travel_date}")
            if return_date:
                print(f"   Date retour: {return_date}")
            
            # API Aviationstack pour chercher des vols r√©els
            base_url = "http://api.aviationstack.com/v1/flights"
            
            flights_data = []
            
            # Rechercher le vol aller
            params = {
                'access_key': api_key,
                'dep_iata': origin_code,
                'arr_iata': destination_code,
                'flight_date': travel_date,  # Format YYYY-MM-DD
            }
            
            print(f"   üì° Requ√™te API Aviationstack pour vol aller...")
            response = requests.get(base_url, params=params, timeout=15)
            
            print(f"   üìä Code de statut: {response.status_code}")
            
            if response.status_code == 200:
                data = response.json()
                
                # V√©rifier les erreurs de l'API
                if data.get('error'):
                    error_info = data.get('error', {})
                    print(f"   ‚ùå Erreur API Aviationstack: {error_info.get('message', 'Erreur inconnue')}")
                    if error_info.get('code') == 104:
                        print(f"   ‚ö†Ô∏è Limite de requ√™tes mensuelle atteinte (500 requ√™tes gratuites)")
                    elif error_info.get('code') == 101:
                        print(f"   ‚ö†Ô∏è Cl√© API invalide - v√©rifiez AVIATIONSTACK_API_KEY dans .env")
                    return None
                
                if data.get('data') and len(data['data']) > 0:
                    print(f"   ‚úÖ {len(data['data'])} vol(s) R√âEL(S) trouv√©(s) pour {origin_code} ‚Üí {destination_code}")
                    
                    # Prendre les 3 premiers vols (directs de pr√©f√©rence)
                    for flight in data['data'][:3]:
                        flight_info = {
                            'flight_number': flight.get('flight', {}).get('number', 'N/A'),
                            'airline': flight.get('airline', {}).get('name', 'N/A'),
                            'departure_airport': flight.get('departure', {}).get('iata', origin_code),
                            'departure_time': flight.get('departure', {}).get('scheduled', 'N/A'),
                            'arrival_airport': flight.get('arrival', {}).get('iata', destination_code),
                            'arrival_time': flight.get('arrival', {}).get('scheduled', 'N/A'),
                            'status': flight.get('flight_status', 'scheduled'),
                            'is_direct': not flight.get('departure', {}).get('airport', '') != flight.get('arrival', {}).get('airport', ''),
                        }
                        flights_data.append(flight_info)
                        print(f"      ‚úàÔ∏è Vol trouv√©: {flight_info['airline']} {flight_info['flight_number']} - {flight_info['departure_time']} ‚Üí {flight_info['arrival_time']}")
                else:
                    print(f"   ‚ö†Ô∏è Aucun vol trouv√© dans la r√©ponse Aviationstack pour {origin_code} ‚Üí {destination_code}")
            else:
                error_text = response.text[:500] if hasattr(response, 'text') else str(response)
                print(f"   ‚ùå Erreur API Aviationstack (HTTP {response.status_code}): {error_text}")
                try:
                    error_data = response.json()
                    if error_data.get('error'):
                        print(f"   üìã D√©tails: {error_data.get('error')}")
                except:
                    pass
            
            # Rechercher le vol retour si date retour fournie
            if return_date and flights_data:
                params_return = {
                    'access_key': api_key,
                    'dep_iata': destination_code,
                    'arr_iata': origin_code,
                    'flight_date': return_date,
                }
                
                print(f"   üì° Requ√™te API Aviationstack pour vol retour...")
                response_return = requests.get(base_url, params=params_return, timeout=15)
                
                if response_return.status_code == 200:
                    data_return = response_return.json()
                    if data_return.get('data') and len(data_return['data']) > 0:
                        # Prendre le premier vol retour
                        flight_return = data_return['data'][0]
                        return_flight_info = {
                            'flight_number': flight_return.get('flight', {}).get('number', 'N/A'),
                            'airline': flight_return.get('airline', {}).get('name', 'N/A'),
                            'departure_airport': flight_return.get('departure', {}).get('iata', destination_code),
                            'departure_time': flight_return.get('departure', {}).get('scheduled', 'N/A'),
                            'arrival_airport': flight_return.get('arrival', {}).get('iata', origin_code),
                            'arrival_time': flight_return.get('arrival', {}).get('scheduled', 'N/A'),
                            'status': flight_return.get('flight_status', 'scheduled'),
                        }
                        flights_data.append(return_flight_info)
                        print(f"      ‚úàÔ∏è Vol retour trouv√©: {return_flight_info['airline']} {return_flight_info['flight_number']}")
            
            if flights_data:
                print(f"‚úÖ {len(flights_data)} vol(s) R√âEL(S) obtenu(s) depuis Aviationstack")
                return flights_data
            else:
                print("‚ö†Ô∏è Aucun vol trouv√© avec Aviationstack")
                return None
                
        except Exception as e:
            print(f"‚ùå Erreur recherche vols Aviationstack: {type(e).__name__}: {str(e)}")
            import traceback
            print(f"   Traceback: {traceback.format_exc()[:300]}")
            return None
    
    def _search_flights_smart(self, flight_input, search_metadata=None):
        """
        Recherche intelligente de vols avec parsing automatique (GDS, num√©ro de vol, texte libre).
        Remplace l'ancienne fonction Air France-KLM.
        
        Args:
            flight_input: Texte saisi par l'utilisateur (format libre)
            search_metadata: Dict pour stocker les m√©tadonn√©es
        
        Returns:
            Liste de dicts avec les vols trouv√©s ou None
        """
        from api.smart_flight_search import SmartFlightSearch
        from django.conf import settings
        
        if not flight_input:
            return None
        
        try:
            use_test = getattr(settings, 'AMADEUS_USE_TEST', True)
            smart_search = SmartFlightSearch(use_test=use_test)
            
            result = smart_search.search(flight_input)
            
            if search_metadata is not None:
                search_metadata['parsed_data'] = result.get('parsed_data')
                search_metadata['search_strategy'] = result.get('search_strategy')
                search_metadata['source'] = 'smart_search_amadeus'
                
                if result.get('flights_found'):
                    search_metadata['real_flights_count'] = len(result['flights_found'])
                    search_metadata['has_valid_flight_info'] = True
                else:
                    search_metadata['real_flights_count'] = 0
                    search_metadata['has_valid_flight_info'] = False
            
            return result.get('flights_found')
        
        except Exception as e:
            print(f"‚ùå Erreur recherche intelligente: {str(e)}")
            import traceback
            traceback.print_exc()
            if search_metadata:
                search_metadata['failure_reason'] = [f'smart_search_error: {str(e)}']
            return None
    
    def _extract_airport_codes(self, text_input, travel_date=None, for_origin=False):
        """
        Extrait les codes d'a√©roport depuis le texte (ex: Paris ‚Üí CDG, Bali ‚Üí DPS)
        Si for_origin=True, extrait le code pour l'origine (premi√®re valeur du tuple)
        Si for_origin=False (d√©faut), extrait le code pour la destination (deuxi√®me valeur du tuple)
        """
        # Mapping villes/destinations ‚Üí codes IATA (plus complet et avec accents)
        # IMPORTANT: Trier par longueur d√©croissante pour d√©tecter d'abord les noms les plus longs
        airport_codes = {
            # France
            'paris': 'CDG', 'cdg': 'CDG', 'orly': 'ORY', 'ory': 'ORY',
            # Belgique
            'belgique': 'BRU', 'bruxelles': 'BRU', 'brussels': 'BRU', 'bru': 'BRU',
            'charleroi': 'CRL', 'crl': 'CRL', 'li√®ge': 'LGG', 'liege': 'LGG', 'lgg': 'LGG',
            'anvers': 'ANR', 'antwerp': 'ANR', 'anr': 'ANR',
            # Indon√©sie
            'bali': 'DPS', 'denpasar': 'DPS', 'dps': 'DPS', 'indonesie': 'DPS', 'indon√©sie': 'DPS',
            'jakarta': 'CGK', 'cgk': 'CGK', 'yogyakarta': 'YIA', 'yia': 'YIA',
            # Tha√Ølande
            'thailande': 'BKK', 'bangkok': 'BKK', 'bkk': 'BKK', 'tha√Ølande': 'BKK',
            'phuket': 'HKT', 'hkt': 'HKT', 'chiang mai': 'CNX', 'chiangmai': 'CNX', 'cnx': 'CNX',
            # Gr√®ce
            'gr√®ce': 'ATH', 'grece': 'ATH', 'ath√®nes': 'ATH', 'athenes': 'ATH', 'ath': 'ATH',
            'mykonos': 'JMK', 'jmk': 'JMK', 'santorin': 'JTR', 'santorini': 'JTR', 'jtr': 'JTR',
            # Italie
            'italie': 'FCO', 'rome': 'FCO', 'fco': 'FCO', 'milan': 'MXP', 'mxp': 'MXP',
            'venise': 'VCE', 'venice': 'VCE', 'vce': 'VCE', 'florence': 'FLR', 'flr': 'FLR',
            # Espagne
            'espagne': 'MAD', 'madrid': 'MAD', 'mad': 'MAD', 'barcelone': 'BCN', 'bcn': 'BCN',
            'seville': 'SVQ', 'sevilla': 'SVQ', 'svq': 'SVQ', 'valencia': 'VLC', 'vlc': 'VLC',
            # Maroc
            'maroc': 'CMN', 'casablanca': 'CMN', 'cmn': 'CMN', 'marrakech': 'RAK', 'rak': 'RAK',
            'agadir': 'AGA', 'aga': 'AGA', 'tanger': 'TNG', 'tangier': 'TNG', 'tng': 'TNG',
            # √âmirats
            'duba√Ø': 'DXB', 'dubai': 'DXB', 'dxb': 'DXB', 'abu dhabi': 'AUH', 'auh': 'AUH',
            # Japon
            'japon': 'NRT', 'tokyo': 'NRT', 'narita': 'NRT', 'nrt': 'NRT', 'osaka': 'KIX',
            # UK
            'londres': 'LHR', 'london': 'LHR', 'lhr': 'LHR',
            # Turquie
            'istanbul': 'IST', 'ist': 'IST',
            # USA
            'new york': 'JFK', 'jfk': 'JFK', 'nyc': 'JFK',
            # Autres destinations populaires
            'lisbonne': 'LIS', 'lis': 'LIS',
            'amsterdam': 'AMS', 'ams': 'AMS',
            'berlin': 'BER', 'ber': 'BER',
            'vienne': 'VIE', 'vie': 'VIE',
            'prague': 'PRG', 'prg': 'PRG',
            'budapest': 'BUD', 'bud': 'BUD',
            'venise': 'VCE', 'vce': 'VCE',
            'florence': 'FLR', 'flr': 'FLR',
            'naples': 'NAP', 'nap': 'NAP',
        }
        
        text_lower = text_input.lower()
        origin_code = 'CDG'  # Paris par d√©faut
        destination_code = None
        
        print(f"   üîç Recherche de codes a√©roport dans: '{text_input[:100]}'")
        
        # Trouver la ville/destination dans le texte
        city_matches = []
        for city, code in airport_codes.items():
            if city in text_lower:
                city_matches.append((city, code))
        
        # Trier par longueur (les noms plus longs sont plus sp√©cifiques)
        city_matches.sort(key=lambda x: len(x[0]), reverse=True)
        
        if city_matches:
            # Prendre la premi√®re correspondance (la plus longue/specifique)
            matched_city, matched_code = city_matches[0]
            
            if for_origin:
                # Si on cherche l'origine, on retourne le code trouv√© comme origine
                origin_code = matched_code
                print(f"   ‚úÖ Origine d√©tect√©e: '{matched_city}' ‚Üí code a√©roport {matched_code}")
            else:
                # Si on cherche la destination, on exclut Paris (c'est l'origine par d√©faut)
                if matched_city != 'paris' and matched_code != 'CDG':
                    destination_code = matched_code
                    print(f"   ‚úÖ Destination d√©tect√©e: '{matched_city}' ‚Üí code a√©roport {matched_code}")
                else:
                    print(f"   ‚ö†Ô∏è 'Paris' d√©tect√© mais c'est l'origine par d√©faut - pas de destination trouv√©e")
        else:
            if for_origin:
                print(f"   ‚ö†Ô∏è Aucune origine connue d√©tect√©e dans le texte - utilisation de Paris (CDG) par d√©faut")
            else:
                print(f"   ‚ùå Aucune destination connue d√©tect√©e dans le texte")
                print(f"   üí° Destinations support√©es: belgique, bali, thailande, gr√®ce, italie, espagne, maroc, duba√Ø, japon, londres, istanbul, new york, etc.")
        
        return origin_code, destination_code
    
    def _search_real_time_info(self, query, max_results=3):
        """
        Recherche d'informations en temps r√©el via Tavily (si disponible).
        Utile pour rechercher des vols, horaires, prix r√©els.
        """
        if not TAVILY_AVAILABLE:
            return None
        
        try:
            tavily_api_key = getattr(settings, 'TAVILY_API_KEY', None)
            if not tavily_api_key:
                print("‚ö†Ô∏è TAVILY_API_KEY non configur√©e dans settings.py")
                return None
            
            tavily = TavilyClient(api_key=tavily_api_key)
            response = tavily.search(
                query=query,
                search_depth="advanced",  # Recherche approfondie
                max_results=max_results,
                include_answer=True,
                include_raw_content=True
            )
            
            results = []
            if response.get('results'):
                for result in response['results'][:max_results]:
                    results.append({
                        "title": result.get('title', ''),
                        "url": result.get('url', ''),
                        "content": result.get('content', '')[:1000],  # Limiter √† 1000 chars
                        "raw_content": result.get('raw_content', '')[:1500] if result.get('raw_content') else ''
                    })
            
            # Ajouter aussi la r√©ponse g√©n√©r√©e par Tavily si disponible
            if response.get('answer'):
                results.append({
                    "title": "R√©ponse synth√©tis√©e",
                    "url": "",
                    "content": response['answer'],
                    "raw_content": ""
                })
            
            return results
        except Exception as e:
            print(f"‚ùå Erreur recherche Tavily: {str(e)}")
            return None
    
    def _scrape_hotels_search_results(self, url):
        """
        Scrape une page de RECHERCHE d'h√¥tels (plusieurs r√©sultats) avec Playwright.
        Extrait les informations de plusieurs h√¥tels pour laisser ChatGPT choisir.
        """
        if not PLAYWRIGHT_AVAILABLE:
            return None
        
        try:
            print(f"üè® Scraping de R√âSULTATS DE RECHERCHE d'h√¥tels avec Playwright pour: {url}")
            
            with sync_playwright() as p:
                print(f"   üöÄ Lancement du navigateur Chromium...")
                browser = p.chromium.launch(headless=True)
                page = browser.new_page()
                
                # User-Agent r√©aliste
                page.set_extra_http_headers({
                    'Accept-Language': 'fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
                })
                
                print(f"   üì° Chargement de la page de recherche...")
                page.goto(url, wait_until='networkidle', timeout=45000)  # Timeout augment√©
                
                # Attendre plus longtemps pour que les r√©sultats se chargent
                print(f"   ‚è±Ô∏è Attente du chargement des r√©sultats d'h√¥tels (8 secondes)...")
                page.wait_for_timeout(8000)
                
                # Extraire le contenu HTML complet
                content = page.content()
                
                # Prendre des screenshots pour debug si n√©cessaire (optionnel)
                # page.screenshot(path=f"debug_search_{int(time.time())}.png")
                
                browser.close()
                
                # Parser avec BeautifulSoup
                soup = BeautifulSoup(content, 'html.parser')
                
                # Supprimer les √©l√©ments inutiles
                for element in soup(["script", "style", "nav", "footer", "header"]):
                    element.decompose()
                
                # Extraire le texte complet (contient infos sur plusieurs h√¥tels)
                text_content = soup.get_text(separator=' ', strip=True)
                
                # Nettoyer
                lines = (line.strip() for line in text_content.splitlines())
                chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                text_content = ' '.join(chunk for chunk in chunks if chunk)
                
                # Construire un r√©sum√© structur√© pour ChatGPT
                result = f"""üè® R√âSULTATS DE RECHERCHE D'H√îTELS (PLUSIEURS OPTIONS DISPONIBLES)

Cette page contient PLUSIEURS H√îTELS disponibles pour les dates recherch√©es. 
ChatGPT doit analyser ces options et choisir le(s) meilleur(s) pour le circuit.

üìã CONTENU EXTRAIT :
{text_content[:5000]}

üí° INSTRUCTIONS POUR CHATGPT :
- Plusieurs h√¥tels sont list√©s ci-dessus avec leurs caract√©ristiques (nom, prix, √©toiles, √©quipements, localisation, notes)
- Analyse TOUTES les options disponibles
- Choisis le(s) meilleur(s) h√¥tel(s) en fonction du rapport qualit√©/prix, localisation, services
- Utilise les NOMS EXACTS, PRIX R√âELS, et DESCRIPTIONS EXACTES des h√¥tels mentionn√©s
- Si plusieurs h√¥tels sont int√©ressants pour diff√©rentes √©tapes du circuit, tu peux les utiliser tous
"""
                
                if text_content and len(text_content.strip()) > 200:
                    print(f"‚úÖ Scraping recherche d'h√¥tels r√©ussi: {len(text_content)} caract√®res")
                    preview = text_content[:300].replace('\n', ' ')
                    print(f"   üìÑ Preview: {preview}...")
                    return result
                else:
                    print(f"‚ö†Ô∏è Contenu recherche trop court: {len(text_content) if text_content else 0} caract√®res")
                    return None
                    
        except Exception as e:
            print(f"‚ùå Erreur scraping recherche d'h√¥tels: {type(e).__name__}: {str(e)}")
            import traceback
            print(f"   Traceback: {traceback.format_exc()[:500]}")
            return None
    
    def _scrape_with_playwright(self, url):
        """
        Utilise Playwright (navigateur headless) pour scraper les sites JavaScript.
        Ex√©cute le JavaScript et r√©cup√®re le contenu rendu.
        
        D√âTECTE automatiquement si c'est une page de recherche (plusieurs h√¥tels)
        ou une page d'h√¥tel unique et adapte le scraping.
        """
        if not PLAYWRIGHT_AVAILABLE:
            return None
        
        # üîç D√©tection automatique : Page de recherche vs Page unique
        is_search_page = any(indicator in url.lower() for indicator in [
            'redirect.htm', 'search', 'results', 'recherche', 'liste', 
            'toPolygonCode', 'startDate', 'endDate', 'nbAdults'
        ])
        
        if is_search_page:
            print(f"üîç D√©tection : PAGE DE RECHERCHE (plusieurs h√¥tels) ‚Üí Mode extraction multiple")
            return self._scrape_hotels_search_results(url)
        else:
            print(f"üîç D√©tection : PAGE UNIQUE (h√¥tel sp√©cifique) ‚Üí Mode extraction classique")
        
        try:
            print(f"üåê Tentative de scraping via Playwright (navigateur headless) pour: {url}")
            
            with sync_playwright() as p:
                print(f"   üöÄ Lancement du navigateur Chromium...")
                browser = p.chromium.launch(headless=True)
                page = browser.new_page()
                
                # D√©finir un User-Agent r√©aliste
                page.set_extra_http_headers({
                    'Accept-Language': 'fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7'
                })
                
                print(f"   üì° Chargement de la page: {url}")
                # Attendre que le contenu soit charg√©
                page.goto(url, wait_until='networkidle', timeout=30000)
                
                # Attendre un peu pour que le JavaScript se charge
                page.wait_for_timeout(2000)  # 2 secondes suppl√©mentaires
                
                # Extraire le contenu texte
                print(f"   üìÑ Extraction du contenu...")
                content = page.content()
                
                # Parser avec BeautifulSoup pour extraire le texte proprement
                soup = BeautifulSoup(content, 'html.parser')
                
                # Supprimer les scripts et styles
                for script in soup(["script", "style", "nav", "footer", "header"]):
                    script.decompose()
                
                # Extraire meta description
                meta_desc = ""
                meta_tag = soup.find("meta", attrs={"name": "description"})
                if meta_tag and meta_tag.get("content"):
                    meta_desc = meta_tag.get("content")
                
                # Extraire le texte principal
                text_content = soup.get_text(separator=' ', strip=True)
                
                # Nettoyer le texte
                lines = (line.strip() for line in text_content.splitlines())
                chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                text_content = ' '.join(chunk for chunk in chunks if chunk)
                
                # Combiner
                full_content = ""
                if meta_desc:
                    full_content = f"Description: {meta_desc}\n\n"
                full_content += text_content
                
                browser.close()
                
                if full_content and len(full_content.strip()) > 50:
                    print(f"‚úÖ Scraping Playwright r√©ussi: {len(full_content)} caract√®res")
                    preview = full_content[:200].replace('\n', ' ')
                    print(f"   üìÑ Preview: {preview}...")
                    return full_content[:3000] if len(full_content) > 3000 else full_content
                else:
                    print(f"‚ö†Ô∏è Contenu Playwright trop court: {len(full_content) if full_content else 0} caract√®res")
                    return None
                    
        except Exception as e:
            print(f"‚ùå Erreur scraping Playwright pour {url}: {type(e).__name__}: {str(e)}")
            import traceback
            print(f"   Traceback: {traceback.format_exc()[:500]}")
            return None
    
    def _extract_images_with_playwright(self, url):
        """
        Extrait les URLs des images depuis une page avec Playwright.
        D√âTECTE automatiquement si c'est une page de recherche ou page unique.
        """
        if not PLAYWRIGHT_AVAILABLE:
            return []
        
        # D√©tection automatique du type de page
        is_search_page = any(indicator in url.lower() for indicator in [
            'redirect.htm', 'search', 'results', 'recherche', 'liste', 
            'toPolygonCode', 'startDate', 'endDate', 'nbAdults'
        ])
        
        limit = 20 if is_search_page else 10  # Plus d'images pour les pages de recherche
        wait_time = 8000 if is_search_page else 2000  # Attendre plus longtemps pour recherche
        
        try:
            print(f"   üñºÔ∏è Extraction des images avec Playwright...")
            if is_search_page:
                print(f"   üè® Page de recherche d√©tect√©e ‚Üí Extraction de {limit} images (plusieurs h√¥tels)")
            
            images = []
            
            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True)
                page = browser.new_page()
                page.set_extra_http_headers({
                    'Accept-Language': 'fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7'
                })
                
                page.goto(url, wait_until='networkidle', timeout=45000)
                page.wait_for_timeout(wait_time)
                
                # Extraire toutes les images
                img_elements = page.query_selector_all('img')
                
                for img in img_elements:
                    src = img.get_attribute('src')
                    if not src:
                        # Essayer data-src (lazy loading)
                        src = img.get_attribute('data-src')
                    if not src:
                        # Essayer data-lazy-src
                        src = img.get_attribute('data-lazy-src')
                    
                    if src:
                        # Convertir les URLs relatives en absolues
                        if src.startswith('//'):
                            src = 'https:' + src
                        elif src.startswith('/'):
                            from urllib.parse import urljoin
                            src = urljoin(url, src)
                        elif not src.startswith('http'):
                            from urllib.parse import urljoin
                            src = urljoin(url, src)
                        
                        # Filtrer les images trop petites (ic√¥nes, logos, etc.)
                        try:
                            width = img.get_attribute('width')
                            height = img.get_attribute('height')
                            if width and height:
                                if int(width) < 100 or int(height) < 100:
                                    continue
                        except:
                            pass
                        
                        # Filtrer les images de tracking/pixel (moins strict pour pages de recherche)
                        skip_patterns = ['pixel', 'tracking', 'analytics', 'beacon']
                        if not is_search_page:
                            skip_patterns.extend(['logo', 'icon'])
                        
                        if any(skip in src.lower() for skip in skip_patterns):
                            continue
                        
                        images.append(src)
                
                browser.close()
                
            # D√©doublonner et limiter
            unique_images = list(dict.fromkeys(images))[:limit]
            print(f"   ‚úÖ {len(unique_images)} image(s) extraite(s)")
            return unique_images
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è Erreur extraction images Playwright: {str(e)}")
            return []
    
    def _extract_images_from_url(self, url):
        """
        Extrait les URLs des images depuis une page avec BeautifulSoup.
        """
        try:
            print(f"   üñºÔ∏è Extraction des images avec BeautifulSoup...")
            images = []
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, headers=headers, timeout=15, allow_redirects=True, verify=False)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                img_tags = soup.find_all('img')
                
                for img in img_tags:
                    src = img.get('src') or img.get('data-src') or img.get('data-lazy-src')
                    if src:
                        # Convertir les URLs relatives en absolues
                        if src.startswith('//'):
                            src = 'https:' + src
                        elif src.startswith('/'):
                            from urllib.parse import urljoin
                            src = urljoin(url, src)
                        elif not src.startswith('http'):
                            from urllib.parse import urljoin
                            src = urljoin(url, src)
                        
                        # Filtrer les images trop petites et de tracking
                        if any(skip in src.lower() for skip in ['pixel', 'tracking', 'analytics', 'beacon', 'logo', 'icon', '1x1']):
                            continue
                        
                        images.append(src)
            
            # Limiter √† 10 images et d√©doublonner
            unique_images = list(dict.fromkeys(images))[:10]
            print(f"   ‚úÖ {len(unique_images)} image(s) extraite(s)")
            return unique_images
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è Erreur extraction images BeautifulSoup: {str(e)}")
            return []
    
    def _scrape_with_tavily(self, url):
        """
        Utilise Tavily pour extraire le contenu d'une URL sp√©cifique.
        Utile pour les sites avec JavaScript ou protection anti-scraping.
        """
        if not TAVILY_AVAILABLE:
            return None
        
        try:
            tavily_api_key = getattr(settings, 'TAVILY_API_KEY', None)
            if not tavily_api_key:
                print(f"   ‚ö†Ô∏è TAVILY_API_KEY non configur√©e")
                return None
            
            print(f"üîç Tentative de scraping via Tavily pour: {url}")
            tavily = TavilyClient(api_key=tavily_api_key)
            
            # Nettoyer l'URL si n√©cessaire (enlever certains param√®tres qui peuvent poser probl√®me)
            clean_url = url
            # Garder l'URL telle quelle pour Tavily car il peut g√©rer les query strings
            
            print(f"   üì° Appel API Tavily extract...")
            # Utiliser la m√©thode extract de Tavily qui g√®re mieux les sites JS
            response = tavily.extract(
                urls=[clean_url],
                extract_depth="advanced",
                format="text",
                include_raw_content=True,
                    timeout=30  # Timeout r√©duit pour √©viter les blocages
            )
            
            print(f"   üì• R√©ponse Tavily re√ßue")
            
            if response and response.get('results') and len(response['results']) > 0:
                result = response['results'][0]
                content = result.get('raw_content') or result.get('content', '')
                
                # Afficher un preview du contenu
                if content:
                    preview = content[:200].replace('\n', ' ')
                    print(f"   üìÑ Preview du contenu: {preview}...")
                
                if content and len(content.strip()) > 50:
                    print(f"‚úÖ Scraping Tavily r√©ussi: {len(content)} caract√®res")
                    # Augmenter la limite √† 3000 caract√®res pour avoir plus d'infos
                    return content[:3000] if len(content) > 3000 else content
                else:
                    print(f"‚ö†Ô∏è Contenu Tavily trop court ou vide: {len(content) if content else 0} caract√®res")
            
            print(f"‚ö†Ô∏è Aucun r√©sultat dans la r√©ponse Tavily")
            return None
        except Exception as e:
            print(f"‚ùå Erreur scraping Tavily pour {url}: {type(e).__name__}: {str(e)}")
            import traceback
            print(f"   Traceback: {traceback.format_exc()[:500]}")
            return None
    
    def _extract_flight_info_from_text(self, text_input, travel_date=None, return_date=None):
        """
        Extrait les informations de vol depuis le texte (dates, destinations, etc.)
        pour d√©clencher une recherche en temps r√©el si n√©cessaire.
        Retourne un tuple (query, metadata) pour tracer ce qui a √©t√© recherch√©.
        """
        flight_keywords = ['vol', 'avion', 'a√©rien', 'd√©part', 'arriv√©e', 'compagnie']
        date_keywords = ['date', 'jour', 'mars', 'avril', 'mai', 'juin', 'juillet', 'ao√ªt', 'septembre', 'octobre', 'novembre', 'd√©cembre']
        
        has_flight = any(keyword in text_input.lower() for keyword in flight_keywords)
        has_date = any(keyword in text_input.lower() for keyword in date_keywords) or travel_date
        
        metadata = {
            'has_flight': has_flight,
            'has_date': has_date,
            'travel_date': travel_date,
            'return_date': return_date,
            'destinations': []
        }
        
        if has_flight and has_date:
            # Construire une query de recherche pour les vols
            # Extraire destination si possible
            destinations = []
            common_destinations = ['paris', 'bali', 'thailande', 'gr√®ce', 'italie', 'espagne', 'maroc', 'tunisie', 'duba√Ø', 'japon', 'tokyo', 'new york', 'londres', 'rome', 'ath√®nes', 'istanbul']
            for dest in common_destinations:
                if dest in text_input.lower():
                    destinations.append(dest)
            
            metadata['destinations'] = destinations
            
            # Construire la query avec la date si disponible
            query_parts = []
            if destinations:
                query_parts.append(f"vols {destinations[0]}")
            else:
                query_parts.append("vols")
            
            # Ajouter la date aller dans la recherche
            if travel_date:
                # Convertir la date au format lisible
                try:
                    from datetime import datetime
                    # G√©rer diff√©rents formats de date
                    date_str_input = str(travel_date).strip()
                    print(f"   Formatage date aller: '{date_str_input}'")
                    
                    # Essayer diff√©rents formats
                    date_formats = ['%Y-%m-%d', '%d/%m/%Y', '%m/%d/%Y', '%Y-%m-%dT%H:%M:%S']
                    date_obj = None
                    for fmt in date_formats:
                        try:
                            date_obj = datetime.strptime(date_str_input.split('T')[0], fmt)
                            break
                        except:
                            continue
                    
                    if date_obj:
                        # Utiliser les mois en fran√ßais
                        months_fr = {
                            1: 'janvier', 2: 'f√©vrier', 3: 'mars', 4: 'avril',
                            5: 'mai', 6: 'juin', 7: 'juillet', 8: 'ao√ªt',
                            9: 'septembre', 10: 'octobre', 11: 'novembre', 12: 'd√©cembre'
                        }
                        date_str = f"{date_obj.day} {months_fr[date_obj.month]} {date_obj.year}"
                        query_parts.append(f"date {date_str}")
                        metadata['travel_date_formatted'] = date_str
                        print(f"   ‚úÖ Date format√©e: {date_str}")
                    else:
                        query_parts.append(f"date {travel_date}")
                        metadata['travel_date_formatted'] = travel_date
                        print(f"   ‚ö†Ô∏è Format non reconnu, utilisation brute: {travel_date}")
                except Exception as e:
                    print(f"   ‚ùå Erreur formatage date: {e}")
                    query_parts.append(f"date {travel_date}")
                    metadata['travel_date_formatted'] = travel_date
            
            # Ajouter la date retour si disponible
            if return_date:
                try:
                    from datetime import datetime
                    # G√©rer diff√©rents formats de date
                    date_str_input = str(return_date).strip()
                    print(f"   Formatage date retour: '{date_str_input}'")
                    
                    # Essayer diff√©rents formats
                    date_formats = ['%Y-%m-%d', '%d/%m/%Y', '%m/%d/%Y', '%Y-%m-%dT%H:%M:%S']
                    date_obj = None
                    for fmt in date_formats:
                        try:
                            date_obj = datetime.strptime(date_str_input.split('T')[0], fmt)
                            break
                        except:
                            continue
                    
                    if date_obj:
                        # Utiliser les mois en fran√ßais
                        months_fr = {
                            1: 'janvier', 2: 'f√©vrier', 3: 'mars', 4: 'avril',
                            5: 'mai', 6: 'juin', 7: 'juillet', 8: 'ao√ªt',
                            9: 'septembre', 10: 'octobre', 11: 'novembre', 12: 'd√©cembre'
                        }
                        date_str = f"{date_obj.day} {months_fr[date_obj.month]} {date_obj.year}"
                        query_parts.append(f"retour {date_str}")
                        metadata['return_date_formatted'] = date_str
                        print(f"   ‚úÖ Date retour format√©e: {date_str}")
                    else:
                        query_parts.append(f"retour {return_date}")
                        metadata['return_date_formatted'] = return_date
                except Exception as e:
                    print(f"   ‚ùå Erreur formatage date retour: {e}")
                    query_parts.append(f"retour {return_date}")
                    metadata['return_date_formatted'] = return_date
                query_parts.append("aller-retour")
            
            query_parts.append("horaires prix disponibilit√©")
            query = " ".join(query_parts)
            
            return query, metadata
        
        return None, metadata
    
    def _get_prompt_circuit(self, text_input, website_descriptions=None, example_templates=None, travel_date=None, return_date=None, real_time_search=None, real_flights_context=None, offer_type="circuit"):
        """Prompt pour un Circuit (plusieurs jours avec itin√©raire)"""
        
        # Ajouter les descriptions des sites web si disponibles
        website_context = ""
        has_search_results = False
        
        if website_descriptions and len(website_descriptions) > 0:
            website_context = "\n\nüìãüìãüìã INFORMATIONS R√âCUP√âR√âES DEPUIS DES SITES WEB (CONTENU R√âEL - UTILISER EN PRIORIT√â ABSOLUE) :\n"
            for idx, desc in enumerate(website_descriptions, 1):
                website_context += f"\n--- Site {idx}: {desc.get('url', 'URL inconnue')} ---\n"
                content = desc.get('content', '')
                
                # D√©tecter si c'est une page de r√©sultats de recherche
                if "R√âSULTATS DE RECHERCHE D'H√îTELS" in content or "PLUSIEURS OPTIONS DISPONIBLES" in content:
                    has_search_results = True
                    website_context += content[:5000] + "\n"  # Plus de caract√®res pour les pages de recherche
                else:
                    website_context += content[:3000] + "\n"
                
                # Ajouter les images si disponibles
                images = desc.get('images', [])
                if images:
                    image_limit = 10 if has_search_results else 5
                    website_context += f"\nüñºÔ∏è IMAGES DISPONIBLES DEPUIS CE SITE ({len(images)} image(s)):\n"
                    for img_idx, img_url in enumerate(images[:image_limit], 1):
                        website_context += f"- Image {img_idx}: {img_url}\n"
                    website_context += "\n‚ö†Ô∏è IMPORTANT : Ces images proviennent du site web. Tu peux les mentionner dans l'offre ou les utiliser pour enrichir les descriptions.\n"
            
            website_context += "\nüö®üö®üö® CRITIQUE - UTILISATION DES SITES WEB :\n"
            website_context += "- Ces informations proviennent DIRECTEMENT du site web scrap√©\n"
            
            if has_search_results:
                website_context += "\nüè®üè®üè® PAGE DE RECHERCHE D√âTECT√âE (PLUSIEURS H√îTELS) :\n"
                website_context += "- Le site contient PLUSIEURS H√îTELS avec leurs caract√©ristiques (nom, prix, √©toiles, localisation, √©quipements, notes)\n"
                website_context += "- ANALYSE toutes les options et CHOISIS le(s) meilleur(s) h√¥tel(s) pour ce circuit\n"
                website_context += "- Crit√®res de s√©lection : rapport qualit√©/prix, emplacement, services, note des voyageurs\n"
                website_context += "- Utilise les NOMS EXACTS, PRIX R√âELS, √âTOILES et DESCRIPTIONS des h√¥tels mentionn√©s\n"
                website_context += "- Pour un circuit multi-√©tapes, tu peux choisir PLUSIEURS h√¥tels diff√©rents si pertinent\n"
                website_context += "- NE CR√âE PAS d'h√¥tels fictifs - utilise UNIQUEMENT ceux list√©s dans les r√©sultats\n"
            else:
                website_context += "- Utilise TOUTES les descriptions, d√©tails, activit√©s mentionn√©es sur le site\n"
            
            website_context += "- Si le site mentionne des temples, plages, activit√©s sp√©cifiques, utilise-les EXACTEMENT\n"
            website_context += "- Si le site mentionne des h√¥tels, zones, lieux sp√©cifiques, utilise-les\n"
            website_context += "- Si le site mentionne des transferts, transport a√©roport-h√¥tel, ou services de transport, utilise-les EXACTEMENT\n"
            website_context += "- Ne cr√©e PAS de nouvelles descriptions - utilise celles du site scrap√©\n"
            website_context += "- Les descriptions du site doivent appara√Ætre dans ton offre, pas des descriptions invent√©es\n"
            website_context += "- Pour l'introduction, utilise les descriptions du site web, pas tes propres descriptions\n"
            website_context += "- Les images fournies peuvent √™tre utilis√©es pour enrichir l'offre (mentionner leur contenu dans les descriptions)\n"
        elif website_descriptions is not None and len(website_descriptions) == 0:
            # Des URLs ont √©t√© fournies mais le scraping a √©chou√©
            website_context = "\n\n‚ö†Ô∏è ATTENTION : Des URLs de sites web ont √©t√© fournies mais n'ont pas pu √™tre scrapp√©es (sites inaccessibles ou protection anti-scraping). Utilise les informations de recherche Tavily ou tes connaissances g√©n√©rales.\n"
        
        # Ajouter les dates explicites dans le prompt
        dates_context = ""
        if travel_date or return_date:
            dates_context = "\n\nüìÖüìÖüìÖ DATES DU VOYAGE (√Ä UTILISER EXACTEMENT - NE PAS INVENTER) :\n"
            if travel_date:
                try:
                    from datetime import datetime
                    months_fr = {
                        1: 'janvier', 2: 'f√©vrier', 3: 'mars', 4: 'avril',
                        5: 'mai', 6: 'juin', 7: 'juillet', 8: 'ao√ªt',
                        9: 'septembre', 10: 'octobre', 11: 'novembre', 12: 'd√©cembre'
                    }
                    date_obj = datetime.strptime(str(travel_date).strip().split('T')[0], '%Y-%m-%d')
                    date_formatted = f"{date_obj.day} {months_fr[date_obj.month]} {date_obj.year}"
                    dates_context += f"- Date de d√©part : {date_formatted} ({travel_date})\n"
                except:
                    dates_context += f"- Date de d√©part : {travel_date}\n"
            if return_date:
                try:
                    from datetime import datetime
                    months_fr = {
                        1: 'janvier', 2: 'f√©vrier', 3: 'mars', 4: 'avril',
                        5: 'mai', 6: 'juin', 7: 'juillet', 8: 'ao√ªt',
                        9: 'septembre', 10: 'octobre', 11: 'novembre', 12: 'd√©cembre'
                    }
                    date_obj = datetime.strptime(str(return_date).strip().split('T')[0], '%Y-%m-%d')
                    date_formatted = f"{date_obj.day} {months_fr[date_obj.month]} {date_obj.year}"
                    dates_context += f"- Date de retour : {date_formatted} ({return_date})\n"
                except:
                    dates_context += f"- Date de retour : {return_date}\n"
            dates_context += "\nüö®üö®üö® IMPORTANT : Utilise CES DATES EXACTEMENT pour les vols. Ne cr√©e PAS de dates diff√©rentes. Les horaires de vol doivent correspondre √† ces dates. Si tu mentionnes des dates dans l'offre, utilise celles-ci, pas d'autres dates.\n"
        
        # Ajouter les exemples de templates si disponibles
        templates_context = ""
        if example_templates:
            templates_context = "\n\nüìù EXEMPLES DE TEMPLATES √Ä COPIER EXACTEMENT (STRUCTURE ET STYLE) :\n"
            for idx, template in enumerate(example_templates, 1):
                templates_context += f"\n--- Exemple Template {idx} ---\n"
                # Si c'est du JSON, on l'affiche tel quel, sinon on prend le texte
                if isinstance(template, dict):
                    templates_context += json.dumps(template, ensure_ascii=False, indent=2)[:2000] + "\n"
                else:
                    templates_context += str(template)[:2000] + "\n"
            templates_context += "\n‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è CRITIQUE : Copie EXACTEMENT la structure, le style, le format, et l'organisation de ces exemples. Utilise le m√™me niveau de d√©tail, les m√™mes types de sections, et le m√™me ton.\n"
        
        # Ajouter UNIQUEMENT les VRAIS vols d'Air France-KLM
        flights_context = ""
        if real_flights_context:
            flights_context = real_flights_context
        
        return f"""Cr√©e une offre de CIRCUIT de plusieurs jours D√âTAILL√âE et PROFESSIONNELLE pour : {text_input}
{website_context}
{dates_context}
{templates_context}
{flights_context}

üö®üö®üö® R√àGLES ABSOLUES :
1. Pour l'introduction : Si des informations de sites web ont √©t√© fournies ci-dessus, utilise-les EXACTEMENT. Copie les descriptions du site, ne cr√©e pas tes propres descriptions.
2. Pour les dates : Utilise UNIQUEMENT les dates fournies dans la section "DATES DU VOYAGE" ci-dessus. N'invente PAS d'autres dates.
3. Pour les vols : 
   - Si des VOLS R√âELS ont √©t√© fournis ci-dessus (section "VOLS R√âELS TROUV√âS"), utilise-les EXACTEMENT (num√©ro de vol, compagnie, horaires, a√©roports)
   - Les dates de d√©part et retour doivent √™tre celles fournies dans "DATES DU VOYAGE", pas d'autres dates
   - Si AUCUN vol r√©el n'a √©t√© fourni, utilise les dates fournies mais n'invente PAS de num√©ros de vol fictifs
4. Pour les transferts : 
   - Si des informations de transferts ont √©t√© trouv√©es dans les sites web scrap√©s, utilise-les EXACTEMENT et cr√©e une section "Transferts"
   - Si AUCUNE information de transfert n'a √©t√© trouv√©e dans les sites scrap√©s, NE CR√âE PAS de section "Transferts" du tout - omets compl√®tement cette section du JSON

IMPORTANT : Sois TR√àS D√âTAILL√â dans chaque section. Inclus des informations sp√©cifiques, des prix, des horaires, des descriptions compl√®tes.

Format JSON strict :
{{
  "title": "Titre accrocheur et m√©morable pour le circuit",
  "introduction": "Description compl√®te et engageante du circuit (3-4 phrases minimum). IMPORTANT : Si des informations de sites web ont √©t√© fournies, utilise-les EXACTEMENT pour cette introduction, pas tes propres descriptions.",
  "sections": [
    {{
      "id": "flights", 
      "type": "Flights", 
      "title": "Transport A√©rien", 
      "body": "D√©tails COMPLETS des vols : compagnie, num√©ros de vol, horaires pr√©cis, classe de service, dur√©e du vol, a√©roports, bagages inclus, repas √† bord, etc. üö®üö®üö® SI DES VOLS R√âELS ONT √âT√â FOURNIS CI-DESSUS (section VOLS R√âELS TROUV√âS), UTILISE LES EXACTEMENT (num√©ro de vol, compagnie, horaires, a√©roports). Sinon, utilise les dates fournies mais n'invente PAS de num√©ros de vol fictifs."
    }},
    {{
      "id": "transfers", 
      "type": "Transfers", 
      "title": "Transferts & Transport", 
      "body": "D√©tails des transferts : type de v√©hicule, dur√©e, horaires, chauffeur, accueil √† l'a√©roport, transport local entre les √©tapes du circuit, etc. üö®üö®üö® SI DES INFORMATIONS DE TRANSFERTS ONT √âT√â FOURNIES CI-DESSUS (section SITES WEB SCRAP√âS), UTILISE LES EXACTEMENT et cr√©e cette section. SI AUCUNE INFO DE TRANSFERT N'A √âT√â TROUV√âE DANS LES SITES SCRAP√âS, NE CR√âE PAS CETTE SECTION DU TOUT - omets-la du JSON."
    }},
    {{
      "id": "itinerary", 
      "type": "Itin√©raire", 
      "title": "Programme du Circuit", 
      "body": "Itin√©raire JOUR PAR JOUR d√©taill√© : Pour chaque jour, indique les visites, activit√©s, excursions, repas inclus, h√©bergements, horaires pr√©cis. Structure : Jour 1 : [d√©tails], Jour 2 : [d√©tails], etc. (minimum 150-200 mots par jour)"
    }},
    {{
      "id": "hotel", 
      "type": "Hotel", 
      "title": "H√©bergement", 
      "body": "Description D√âTAILL√âE des h√©bergements : nom, cat√©gorie, localisation pour chaque √©tape du circuit, type de chambre, pension, √©quipements, services, vue, etc."
    }},
    {{
      "id": "activities", 
      "type": "Activities", 
      "title": "Activit√©s & Excursions", 
      "body": "Programme d√©taill√© : visites guid√©es, excursions incluses dans le circuit, activit√©s optionnelles, guides, dur√©e, horaires, etc."
    }},
    {{
      "id": "price", 
      "type": "Price", 
      "title": "Tarifs & Conditions", 
      "body": "Prix d√©taill√© par personne, suppl√©ments, conditions de r√©servation, acompte, annulation, assurance, etc."
    }}
  ],
  "cta": {{
    "title": "R√©servez votre circuit de r√™ve !", 
    "description": "Offre limit√©e - Ne manquez pas cette opportunit√© unique", 
    "buttonText": "R√©server maintenant"
  }}
}}

EXIGENCES SP√âCIFIQUES CIRCUIT :
- L'itin√©raire doit √™tre d√©taill√© JOUR PAR JOUR avec toutes les activit√©s, visites, et repas
- Inclus tous les transports entre les diff√©rentes √©tapes du circuit
- D√©cris chaque h√©bergement pour chaque √©tape
- Chaque section doit contenir au moins 150-200 mots de contenu d√©taill√©
- Sois professionnel mais engageant
- Inclus des d√©tails sur les services, √©quipements, et conditions

‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è FORMAT JSON CRITIQUE ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è
- R√©ponds UNIQUEMENT avec un JSON valide, sans texte avant ou apr√®s
- Utilise TOUJOURS des guillemets doubles " pour les cha√Ænes, jamais d'apostrophes '
- √âchappe les guillemets dans le texte avec \\"
- V√©rifie que toutes les accolades {{ et }} sont bien ferm√©es
- V√©rifie qu'il n'y a pas de virgule apr√®s le dernier √©l√©ment d'un tableau ou objet"""

    def _get_prompt_sejour(self, text_input, website_descriptions=None, example_templates=None, travel_date=None, return_date=None, real_time_search=None, real_flights_context=None, offer_type="sejour"):
        """Prompt pour un S√©jour (transport et/ou h√¥tel)"""
        
        # Ajouter les descriptions des sites web si disponibles
        website_context = ""
        if website_descriptions and len(website_descriptions) > 0:
            website_context = "\n\nüìãüìãüìã INFORMATIONS R√âCUP√âR√âES DEPUIS DES SITES WEB (CONTENU R√âEL - UTILISER EN PRIORIT√â ABSOLUE) :\n"
            for idx, desc in enumerate(website_descriptions, 1):
                website_context += f"\n--- Site {idx}: {desc.get('url', 'URL inconnue')} ---\n"
                content = desc.get('content', '')
                website_context += content[:3000] + "\n"  # Augmenter √† 3000 caract√®res pour avoir plus d'infos
                
                # Ajouter les images si disponibles
                images = desc.get('images', [])
                if images:
                    website_context += f"\nüñºÔ∏è IMAGES DISPONIBLES DEPUIS CE SITE ({len(images)} image(s)):\n"
                    for img_idx, img_url in enumerate(images[:5], 1):  # Limiter √† 5 images
                        website_context += f"- Image {img_idx}: {img_url}\n"
                    website_context += "\n‚ö†Ô∏è IMPORTANT : Ces images proviennent du site web. Tu peux les mentionner dans l'offre ou les utiliser pour enrichir les descriptions.\n"
            
            website_context += "\nüö®üö®üö® CRITIQUE - UTILISATION DES SITES WEB :\n"
            website_context += "- Ces informations proviennent DIRECTEMENT du site web scrap√©\n"
            website_context += "- Utilise TOUTES les descriptions, d√©tails, activit√©s mentionn√©es sur le site\n"
            website_context += "- Si le site mentionne des temples, plages, activit√©s sp√©cifiques, utilise-les EXACTEMENT\n"
            website_context += "- Si le site mentionne des h√¥tels, zones, lieux sp√©cifiques, utilise-les\n"
            website_context += "- Si le site mentionne des transferts, transport a√©roport-h√¥tel, ou services de transport, utilise-les EXACTEMENT\n"
            website_context += "- Ne cr√©e PAS de nouvelles descriptions - utilise celles du site scrap√©\n"
            website_context += "- Les descriptions du site doivent appara√Ætre dans ton offre, pas des descriptions invent√©es\n"
            website_context += "- Pour l'introduction, utilise les descriptions du site web, pas tes propres descriptions\n"
            website_context += "- Les images fournies peuvent √™tre utilis√©es pour enrichir l'offre (mentionner leur contenu dans les descriptions)\n"
        elif website_descriptions is not None and len(website_descriptions) == 0:
            # Des URLs ont √©t√© fournies mais le scraping a √©chou√©
            website_context = "\n\n‚ö†Ô∏è ATTENTION : Des URLs de sites web ont √©t√© fournies mais n'ont pas pu √™tre scrapp√©es (sites inaccessibles ou protection anti-scraping). Utilise les informations de recherche Tavily ou tes connaissances g√©n√©rales.\n"
        
        # Ajouter les dates explicites dans le prompt
        dates_context = ""
        if travel_date or return_date:
            dates_context = "\n\nüìÖüìÖüìÖ DATES DU VOYAGE (√Ä UTILISER EXACTEMENT - NE PAS INVENTER) :\n"
            if travel_date:
                try:
                    from datetime import datetime
                    months_fr = {
                        1: 'janvier', 2: 'f√©vrier', 3: 'mars', 4: 'avril',
                        5: 'mai', 6: 'juin', 7: 'juillet', 8: 'ao√ªt',
                        9: 'septembre', 10: 'octobre', 11: 'novembre', 12: 'd√©cembre'
                    }
                    date_obj = datetime.strptime(str(travel_date).strip().split('T')[0], '%Y-%m-%d')
                    date_formatted = f"{date_obj.day} {months_fr[date_obj.month]} {date_obj.year}"
                    dates_context += f"- Date de d√©part : {date_formatted} ({travel_date})\n"
                except:
                    dates_context += f"- Date de d√©part : {travel_date}\n"
            if return_date:
                try:
                    from datetime import datetime
                    months_fr = {
                        1: 'janvier', 2: 'f√©vrier', 3: 'mars', 4: 'avril',
                        5: 'mai', 6: 'juin', 7: 'juillet', 8: 'ao√ªt',
                        9: 'septembre', 10: 'octobre', 11: 'novembre', 12: 'd√©cembre'
                    }
                    date_obj = datetime.strptime(str(return_date).strip().split('T')[0], '%Y-%m-%d')
                    date_formatted = f"{date_obj.day} {months_fr[date_obj.month]} {date_obj.year}"
                    dates_context += f"- Date de retour : {date_formatted} ({return_date})\n"
                except:
                    dates_context += f"- Date de retour : {return_date}\n"
            dates_context += "\nüö®üö®üö® IMPORTANT : Utilise CES DATES EXACTEMENT pour les vols. Ne cr√©e PAS de dates diff√©rentes. Les horaires de vol doivent correspondre √† ces dates. Si tu mentionnes des dates dans l'offre, utilise celles-ci, pas d'autres dates.\n"
        
        # Ajouter les exemples de templates si disponibles
        templates_context = ""
        if example_templates:
            templates_context = "\n\nüìù EXEMPLES DE TEMPLATES √Ä COPIER EXACTEMENT (STRUCTURE ET STYLE) :\n"
            for idx, template in enumerate(example_templates, 1):
                templates_context += f"\n--- Exemple Template {idx} ---\n"
                if isinstance(template, dict):
                    templates_context += json.dumps(template, ensure_ascii=False, indent=2)[:2000] + "\n"
                else:
                    templates_context += str(template)[:2000] + "\n"
            templates_context += "\n‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è CRITIQUE : Copie EXACTEMENT la structure, le style, le format, et l'organisation de ces exemples. Utilise le m√™me niveau de d√©tail, les m√™mes types de sections, et le m√™me ton.\n"
        
        # Ajouter UNIQUEMENT les VRAIS vols d'Air France-KLM
        flights_context = ""
        if real_flights_context:
            flights_context = real_flights_context
        
        return f"""Cr√©e une offre de S√âJOUR (transport et/ou h√¥tel) D√âTAILL√âE et PROFESSIONNELLE pour : {text_input}
{website_context}
{dates_context}
{templates_context}
{flights_context}

üö®üö®üö® R√àGLES ABSOLUES :
1. Pour l'introduction : Si des informations de sites web ont √©t√© fournies ci-dessus, utilise-les EXACTEMENT. Copie les descriptions du site, ne cr√©e pas tes propres descriptions.
2. Pour les dates : Utilise UNIQUEMENT les dates fournies dans la section "DATES DU VOYAGE" ci-dessus. N'invente PAS d'autres dates.
3. Pour les vols : 
   - Si des VOLS R√âELS ont √©t√© fournis ci-dessus (section "VOLS R√âELS TROUV√âS"), utilise-les EXACTEMENT (num√©ro de vol, compagnie, horaires, a√©roports)
   - Les dates de d√©part et retour doivent √™tre celles fournies dans "DATES DU VOYAGE", pas d'autres dates
   - Si AUCUN vol r√©el n'a √©t√© fourni, utilise les dates fournies mais n'invente PAS de num√©ros de vol fictifs
4. Pour les transferts : 
   - Si des informations de transferts ont √©t√© trouv√©es dans les sites web scrap√©s, utilise-les EXACTEMENT et cr√©e une section "Transferts"
   - Si AUCUNE information de transfert n'a √©t√© trouv√©e dans les sites scrap√©s, NE CR√âE PAS de section "Transferts" du tout - omets compl√®tement cette section du JSON

IMPORTANT : Sois TR√àS D√âTAILL√â dans chaque section. Inclus des informations sp√©cifiques, des prix, des horaires, des descriptions compl√®tes.

Format JSON strict :
{{
  "title": "Titre accrocheur et m√©morable pour le s√©jour",
  "introduction": "Description compl√®te et engageante du s√©jour (3-4 phrases minimum). IMPORTANT : Si des informations de sites web ont √©t√© fournies, utilise-les EXACTEMENT pour cette introduction, pas tes propres descriptions.",
  "sections": [
    {{
      "id": "flights",
      "type": "Flights", 
      "title": "Transport A√©rien", 
      "body": "D√©tails COMPLETS des vols : compagnie, num√©ros de vol, horaires pr√©cis, classe de service, dur√©e du vol, a√©roports, bagages inclus, repas √† bord, etc. üö®üö®üö® SI DES VOLS R√âELS ONT √âT√â FOURNIS CI-DESSUS (section VOLS R√âELS TROUV√âS), UTILISE LES EXACTEMENT (num√©ro de vol, compagnie, horaires, a√©roports). Sinon, utilise les dates fournies mais n'invente PAS de num√©ros de vol fictifs."
    }},
    {{
      "id": "transfers", 
      "type": "Transfers", 
      "title": "Transferts", 
      "body": "D√©tails des transferts a√©roport-h√¥tel : type de v√©hicule, dur√©e, horaires, chauffeur, accueil √† l'a√©roport, etc. üö®üö®üö® SI DES INFORMATIONS DE TRANSFERTS ONT √âT√â FOURNIES CI-DESSUS (section SITES WEB SCRAP√âS), UTILISE LES EXACTEMENT et cr√©e cette section. SI AUCUNE INFO DE TRANSFERT N'A √âT√â TROUV√âE DANS LES SITES SCRAP√âS, NE CR√âE PAS CETTE SECTION DU TOUT - omets-la du JSON."
    }},
    {{
      "id": "hotel", 
      "type": "Hotel", 
      "title": "H√©bergement", 
      "body": "Description D√âTAILL√âE de l'h√¥tel : nom, cat√©gorie, localisation, type de chambre, pension (petit-d√©jeuner, demi-pension, pension compl√®te), √©quipements, services, vue, piscine, spa, etc."
    }},
    {{
      "id": "services", 
      "type": "Services", 
      "title": "Services Inclus", 
      "body": "D√©tails des services inclus dans le s√©jour : repas, acc√®s aux √©quipements, activit√©s sur place, etc."
    }},
    {{
      "id": "price", 
      "type": "Price", 
      "title": "Tarifs & Conditions", 
      "body": "Prix d√©taill√© par personne, par nuit, suppl√©ments, conditions de r√©servation, acompte, annulation, assurance, etc."
    }}
  ],
  "cta": {{
    "title": "R√©servez votre s√©jour de r√™ve !", 
    "description": "Offre limit√©e - Ne manquez pas cette opportunit√© unique", 
    "buttonText": "R√©server maintenant"
  }}
}}

EXIGENCES SP√âCIFIQUES S√âJOUR :
- Focus sur l'h√©bergement et le transport (vols + transferts)
- D√©cris en d√©tail l'h√¥tel : chambres, services, √©quipements, restauration
- Chaque section doit contenir au moins 150-200 mots de contenu d√©taill√©
- Sois professionnel mais engageant
- Inclus des d√©tails sur les services, √©quipements, et conditions

‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è FORMAT JSON CRITIQUE ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è
- R√©ponds UNIQUEMENT avec un JSON valide, sans texte avant ou apr√®s
- Utilise TOUJOURS des guillemets doubles " pour les cha√Ænes, jamais d'apostrophes '
- √âchappe les guillemets dans le texte avec \\"
- V√©rifie que toutes les accolades {{ et }} sont bien ferm√©es
- V√©rifie qu'il n'y a pas de virgule apr√®s le dernier √©l√©ment d'un tableau ou objet"""

    def _get_prompt_transport(self, text_input, website_descriptions=None, example_templates=None, real_time_search=None, travel_date=None, return_date=None, real_flights_context=None, offer_type="transport"):
        """Prompt pour Transport seul"""
        
        # Ajouter les descriptions des sites web si disponibles
        website_context = ""
        if website_descriptions and len(website_descriptions) > 0:
            website_context = "\n\nüìãüìãüìã INFORMATIONS R√âCUP√âR√âES DEPUIS DES SITES WEB (CONTENU R√âEL - UTILISER EN PRIORIT√â ABSOLUE) :\n"
            for idx, desc in enumerate(website_descriptions, 1):
                website_context += f"\n--- Site {idx}: {desc.get('url', 'URL inconnue')} ---\n"
                content = desc.get('content', '')
                website_context += content[:3000] + "\n"  # Augmenter √† 3000 caract√®res pour avoir plus d'infos
                
                # Ajouter les images si disponibles
                images = desc.get('images', [])
                if images:
                    website_context += f"\nüñºÔ∏è IMAGES DISPONIBLES DEPUIS CE SITE ({len(images)} image(s)):\n"
                    for img_idx, img_url in enumerate(images[:5], 1):  # Limiter √† 5 images
                        website_context += f"- Image {img_idx}: {img_url}\n"
                    website_context += "\n‚ö†Ô∏è IMPORTANT : Ces images proviennent du site web. Tu peux les mentionner dans l'offre ou les utiliser pour enrichir les descriptions.\n"
            
            website_context += "\nüö®üö®üö® CRITIQUE - UTILISATION DES SITES WEB :\n"
            website_context += "- Ces informations proviennent DIRECTEMENT du site web scrap√©\n"
            website_context += "- Utilise TOUTES les descriptions, d√©tails, activit√©s mentionn√©es sur le site\n"
            website_context += "- Si le site mentionne des temples, plages, activit√©s sp√©cifiques, utilise-les EXACTEMENT\n"
            website_context += "- Si le site mentionne des h√¥tels, zones, lieux sp√©cifiques, utilise-les\n"
            website_context += "- Si le site mentionne des transferts, transport a√©roport-h√¥tel, ou services de transport, utilise-les EXACTEMENT\n"
            website_context += "- Ne cr√©e PAS de nouvelles descriptions - utilise celles du site scrap√©\n"
            website_context += "- Les descriptions du site doivent appara√Ætre dans ton offre, pas des descriptions invent√©es\n"
            website_context += "- Pour l'introduction, utilise les descriptions du site web, pas tes propres descriptions\n"
            website_context += "- Les images fournies peuvent √™tre utilis√©es pour enrichir l'offre (mentionner leur contenu dans les descriptions)\n"
        elif website_descriptions is not None and len(website_descriptions) == 0:
            # Des URLs ont √©t√© fournies mais le scraping a √©chou√©
            website_context = "\n\n‚ö†Ô∏è ATTENTION : Des URLs de sites web ont √©t√© fournies mais n'ont pas pu √™tre scrapp√©es (sites inaccessibles ou protection anti-scraping). Utilise les informations de recherche Tavily ou tes connaissances g√©n√©rales.\n"
        
        # Ajouter les dates explicites dans le prompt
        dates_context = ""
        if travel_date or return_date:
            dates_context = "\n\nüìÖüìÖüìÖ DATES DU VOYAGE (√Ä UTILISER EXACTEMENT - NE PAS INVENTER) :\n"
            if travel_date:
                try:
                    from datetime import datetime
                    months_fr = {
                        1: 'janvier', 2: 'f√©vrier', 3: 'mars', 4: 'avril',
                        5: 'mai', 6: 'juin', 7: 'juillet', 8: 'ao√ªt',
                        9: 'septembre', 10: 'octobre', 11: 'novembre', 12: 'd√©cembre'
                    }
                    date_obj = datetime.strptime(str(travel_date).strip().split('T')[0], '%Y-%m-%d')
                    date_formatted = f"{date_obj.day} {months_fr[date_obj.month]} {date_obj.year}"
                    dates_context += f"- Date de d√©part : {date_formatted} ({travel_date})\n"
                except:
                    dates_context += f"- Date de d√©part : {travel_date}\n"
            if return_date:
                try:
                    from datetime import datetime
                    months_fr = {
                        1: 'janvier', 2: 'f√©vrier', 3: 'mars', 4: 'avril',
                        5: 'mai', 6: 'juin', 7: 'juillet', 8: 'ao√ªt',
                        9: 'septembre', 10: 'octobre', 11: 'novembre', 12: 'd√©cembre'
                    }
                    date_obj = datetime.strptime(str(return_date).strip().split('T')[0], '%Y-%m-%d')
                    date_formatted = f"{date_obj.day} {months_fr[date_obj.month]} {date_obj.year}"
                    dates_context += f"- Date de retour : {date_formatted} ({return_date})\n"
                except:
                    dates_context += f"- Date de retour : {return_date}\n"
            dates_context += "\nüö®üö®üö® IMPORTANT : Utilise CES DATES EXACTEMENT pour les vols. Ne cr√©e PAS de dates diff√©rentes. Les horaires de vol doivent correspondre √† ces dates. Si tu mentionnes des dates dans l'offre, utilise celles-ci, pas d'autres dates.\n"
        
        # Ajouter UNIQUEMENT les VRAIS vols d'Air France-KLM (plus de Tavily)
        if real_flights_context:
            real_time_context = real_flights_context
        else:
            # Pas de vols trouv√©s - contexte vide (pas de Tavily)
            real_time_context = ""
        
        # Ajouter les exemples de templates si disponibles
        templates_context = ""
        if example_templates:
            templates_context = "\n\nüìù EXEMPLES DE TEMPLATES √Ä COPIER EXACTEMENT (STRUCTURE ET STYLE) :\n"
            for idx, template in enumerate(example_templates, 1):
                templates_context += f"\n--- Exemple Template {idx} ---\n"
                if isinstance(template, dict):
                    templates_context += json.dumps(template, ensure_ascii=False, indent=2)[:2000] + "\n"
                else:
                    templates_context += str(template)[:2000] + "\n"
            templates_context += "\n‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è CRITIQUE : Copie EXACTEMENT la structure, le style, le format, et l'organisation de ces exemples. Utilise le m√™me niveau de d√©tail, les m√™mes types de sections, et le m√™me ton.\n"
        
        # Instructions sp√©ciales pour les vols avec dates/heures
        flight_instructions = ""
        has_flight_keywords = any(keyword in text_input.lower() for keyword in ['date', 'heure', 'jour', 'd√©part', 'arriv√©e', 'vol'])
        
        # V√©rifier si on a des informations r√©elles de vols depuis les sites ou Tavily
        has_real_flight_info = False
        if website_descriptions:
            for desc in website_descriptions:
                content = desc.get('content', '').lower()
                # V√©rifier qu'il y a vraiment des infos de vol (pas juste des mots-cl√©s n√©gatifs)
                if any(keyword in content for keyword in ['vol', 'flight', 'compagnie', 'airline', 'd√©part', 'arriv√©e', 'a√©roport']):
                    # Exclure les messages n√©gatifs
                    negative = any(indicator in content for indicator in ['aucun vol', 'no flights', 'pas de vol', 'not available'])
                    if not negative:
                        has_real_flight_info = True
                        break
        
        # Pour Tavily, v√©rifier que les r√©sultats sont valides (pas juste "aucun vol trouv√©")
        if real_time_search and isinstance(real_time_search, list):
            for result in real_time_search:
                content = (result.get('content', '') or result.get('raw_content', '')).lower()
                # V√©rifier qu'il y a des infos positives
                has_positive = any(keyword in content for keyword in ['vol', 'flight', 'departure', 'd√©part', 'arrival', 'arriv√©e', 'airline', 'compagnie'])
                has_negative = any(indicator in content for indicator in ['aucun vol', 'no flights', 'no results', 'pas de vol', 'not available'])
                
                if has_positive and not has_negative:
                    has_real_flight_info = True
                    break
        
        if has_flight_keywords or real_time_search or has_real_flight_info:
            flight_instructions = "\n\n‚úàÔ∏è INSTRUCTIONS SP√âCIALES POUR LES VOLS :\n"
            
            if not has_real_flight_info and offer_type == "transport":
                flight_instructions += "üö®üö®üö®üö®üö® CRITIQUE ABSOLUE - TRANSPORT SEUL :\n"
                flight_instructions += "- AUCUNE information de vol r√©elle trouv√©e dans les sites web scrap√©s ni dans les recherches Tavily\n"
                flight_instructions += "- INTERDICTION TOTALE : NE CR√âE ABSOLUMENT PAS de section de vol avec des informations invent√©es\n"
                flight_instructions += "- NE CR√âE PAS de section de type 'Flights' si tu n'as pas d'informations R√âELLES\n"
                flight_instructions += "- N'INVENTE JAMAIS de num√©ros de vol (UA 123, AF 456, etc.)\n"
                flight_instructions += "- N'INVENTE JAMAIS d'horaires (22h00, 17h30, etc.)\n"
                flight_instructions += "- N'INVENTE JAMAIS de compagnies a√©riennes\n"
                flight_instructions += "- Si tu ne peux pas cr√©er une offre de transport avec des informations R√âELLES, cr√©e UNE SEULE section 'Avertissement' qui dit : '‚ö†Ô∏è Les informations de vol pour cette route/date ne sont pas disponibles. Veuillez v√©rifier directement aupr√®s des compagnies a√©riennes.'\n"
                flight_instructions += "- C'est UN PROBL√àME GRAVE si tu inventes des vols - ces informations seront donn√©es √† des clients r√©els\n"
            elif real_time_search:
                flight_instructions += "‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è ATTENTION : Des recherches en temps r√©el ont √©t√© effectu√©es. Utilise EN PRIORIT√â les informations r√©elles trouv√©es dans les r√©sultats de recherche ci-dessus (horaires, prix, compagnies, disponibilit√©s).\n"
            
            if has_real_flight_info:
                flight_instructions += "- Utilise UNIQUEMENT les informations de vols trouv√©es dans les sites web scrap√©s ou les recherches Tavily\n"
                flight_instructions += "- Si des dates, heures ou jours sont mentionn√©s dans les donn√©es scrap√©es, utilise-les EXACTEMENT comme indiqu√©\n"
                flight_instructions += "- Pour les vols, structure les informations de mani√®re r√©aliste : num√©ro de vol (ex: AF 1234), compagnie, horaires pr√©cis (d√©part/arriv√©e), a√©roports (codes IATA si possible), dur√©e de vol\n"
                flight_instructions += "- Utilise des compagnies a√©riennes r√©elles qui desservent la route mentionn√©e\n"
                flight_instructions += "- Les horaires doivent √™tre coh√©rents avec les fuseaux horaires et les dur√©es de vol r√©elles\n"
        
        return f"""Cr√©e une offre de TRANSPORT SEUL D√âTAILL√âE et PROFESSIONNELLE pour : {text_input}
{website_context}
{dates_context}
{templates_context}
{real_time_context}
{flight_instructions}

üö®üö®üö® R√àGLES ABSOLUES :
1. Pour l'introduction : Si des informations de sites web ont √©t√© fournies ci-dessus, utilise-les EXACTEMENT. Copie les descriptions du site, ne cr√©e pas tes propres descriptions.
2. Pour les dates : Utilise UNIQUEMENT les dates fournies dans la section "DATES DU VOYAGE" ci-dessus. N'invente PAS d'autres dates.
3. Pour les vols : Les dates de d√©part et retour doivent √™tre celles fournies dans "DATES DU VOYAGE", pas d'autres dates. Les horaires peuvent venir des recherches Tavily, mais les DATES doivent √™tre celles fournies.
4. üö®üö®üö®üö®üö® R√àGLE ABSOLUE - INTERDICTION TOTALE D'INVENTER DES VOLS : 
   - SI les recherches Tavily montrent "aucun vol trouv√©", "no flights available", "pas de vol disponible" ‚Üí INTERDICTION TOTALE de cr√©er une section de vol avec des infos invent√©es.
   - SI les sites web scrap√©s n'ont PAS d'informations de vol r√©elles ‚Üí NE CR√âE PAS de section de vol avec des donn√©es fictives.
   - NE JAMAIS inventer de num√©ros de vol (UA 123, AF 456, etc.) - C'EST TR√àS GRAVE, ces infos vont √† des clients r√©els.
   - NE JAMAIS inventer d'horaires (22h00, 17h30, etc.) - C'EST MENTIR aux clients.
   - NE JAMAIS inventer de compagnies a√©riennes ou de routes - C'EST ILL√âGAL de mentir aux clients.
   - SI tu n'as PAS d'informations R√âELLES de vol ‚Üí NE CR√âE PAS de section "Flights" du tout, OU cr√©e UNE section "Avertissement" qui dit : "‚ö†Ô∏è Les informations de vol ne sont pas disponibles pour cette route/date. Contactez directement les compagnies a√©riennes pour v√©rifier les disponibilit√©s et horaires."
   - C'EST UN PROBL√àME CRITIQUE si tu inventes des vols - tu mets des clients en danger avec de fausses informations.
   - Si les sources indiquent "aucun vol trouv√©", tu DOIS respecter √ßa et NE PAS cr√©er de section de vol invent√©e.

IMPORTANT : Sois TR√àS D√âTAILL√â dans chaque section. Inclus des informations sp√©cifiques, des prix, des horaires, des descriptions compl√®tes. MAIS n'invente RIEN qui ne soit pas dans les donn√©es fournies.

Format JSON strict :
{{
  "title": "Titre accrocheur et m√©morable pour le transport",
  "introduction": "Description compl√®te et engageante du service de transport (3-4 phrases minimum). IMPORTANT : Si des informations de sites web ont √©t√© fournies, utilise-les EXACTEMENT pour cette introduction, pas tes propres descriptions.",
  "sections": [
    {{
      "id": "flights", 
      "type": "Flights", 
      "title": "Transport A√©rien", 
      "body": "D√©tails COMPLETS des vols : compagnie a√©rienne, num√©ros de vol, horaires pr√©cis (d√©part et arriv√©e), classe de service (√âconomique, Premium, Affaires, Premi√®re), dur√©e du vol, a√©roports de d√©part et d'arriv√©e, terminal, bagages inclus (cabine et soute), repas √† bord, √©quipements, si√®ges, etc. üö®üö®üö® SI DES VOLS R√âELS ONT √âT√â FOURNIS CI-DESSUS (section VOLS R√âELS TROUV√âS), UTILISE LES EXACTEMENT. Sinon, utilise les dates fournies mais n'invente PAS de num√©ros de vol fictifs."
    }},
    {{
      "id": "baggage", 
      "type": "Bagage", 
      "title": "Bagages", 
      "body": "D√©tails COMPLETS sur les bagages : poids et dimensions autoris√©s pour bagage cabine, bagage en soute, frais suppl√©mentaires, restrictions, etc."
    }},
    {{
      "id": "services", 
      "type": "Services", 
      "title": "Services √† Bord", 
      "body": "Description des services inclus : repas, boissons, divertissement, Wi-Fi, prises √©lectriques, espace pour les jambes, √©quipements de la compagnie, etc."
    }},
    {{
      "id": "price", 
      "type": "Price", 
      "title": "Tarifs & Conditions", 
      "body": "Prix d√©taill√© par personne, par classe de service, suppl√©ments (bagages, si√®ge, repas), conditions de r√©servation, acompte, annulation, modification, assurance, etc."
    }}
  ],
  "cta": {{
    "title": "R√©servez votre transport !", 
    "description": "Offre limit√©e - Ne manquez pas cette opportunit√© unique", 
    "buttonText": "R√©server maintenant"
  }}
}}

EXIGENCES SP√âCIFIQUES TRANSPORT :
- Focus UNIQUEMENT sur le transport (vols a√©riens)
- D√©taille TOUT sur les vols : horaires, compagnies, classes, bagages, services
- Chaque section doit contenir au moins 150-200 mots de contenu d√©taill√©
- Sois professionnel mais engageant
- Inclus tous les d√©tails pratiques pour le transport

‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è FORMAT JSON CRITIQUE ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è
- R√©ponds UNIQUEMENT avec un JSON valide, sans texte avant ou apr√®s
- Utilise TOUJOURS des guillemets doubles " pour les cha√Ænes, jamais d'apostrophes '
- √âchappe les guillemets dans le texte avec \\"
- V√©rifie que toutes les accolades {{ et }} sont bien ferm√©es
- V√©rifie qu'il n'y a pas de virgule apr√®s le dernier √©l√©ment d'un tableau ou objet"""

    def post(self, request):
        text_input = request.data.get("text")
        offer_type = request.data.get("offer_type", "circuit")  # Par d√©faut circuit
        flight_input = request.data.get("flight_input")  # ‚úÖ Infos de vol (format GDS, num√©ro, texte libre)
        company_info = request.data.get("company_info", {})
        website_urls = request.data.get("website_urls", [])  # ‚úÖ Liste d'URLs de sites √† scraper
        example_templates = request.data.get("example_templates", [])  # ‚úÖ Exemples de templates
        
        # Les champs origin, destination, travel_date, return_date peuvent encore √™tre fournis pour compatibilit√©
        # mais ne sont plus affich√©s dans le frontend (extraits automatiquement depuis flight_input)
        origin = request.data.get("origin", "Paris")
        destination = request.data.get("destination")
        travel_date = request.data.get("travel_date")
        return_date = request.data.get("return_date")
        
        print(f"üì• Requ√™te re√ßue:")
        print(f"   - offer_type: {offer_type}")
        print(f"   - flight_input: {flight_input[:100] if flight_input else 'Non fourni'}{'...' if flight_input and len(flight_input) > 100 else ''}")
        print(f"   - text_input longueur: {len(text_input) if text_input else 0} caract√®res")
        if travel_date or destination:
            print(f"   - Params legacy (si fournis): date={travel_date}, dest={destination}")
        
        # Validation : soit text_input, soit flight_input doit √™tre fourni
        if not text_input and not flight_input:
            return Response({"error": "Texte ou infos de vol requis"}, status=status.HTTP_400_BAD_REQUEST)
        
        # Si seulement flight_input est fourni, cr√©er un text_input minimal
        if not text_input and flight_input:
            text_input = f"Demande de devis pour le(s) vol(s) suivant(s) : {flight_input}"
            print(f"   ‚ÑπÔ∏è Texte auto-g√©n√©r√© depuis flight_input")
        
        try:
            # R√©cup√©rer les descriptions des sites web si des URLs sont fournies
            # Limiter √† 3 URLs max pour √©viter les timeouts
            website_descriptions = None
            if website_urls and isinstance(website_urls, list) and len(website_urls) > 0:
                limited_urls = website_urls[:3]  # Maximum 3 URLs pour √©viter les timeouts
                if len(website_urls) > 3:
                    print(f"‚ö†Ô∏è Limitation √† 3 URLs (sur {len(website_urls)}) pour √©viter les timeouts")
                print(f"üåê R√©cup√©ration des descriptions depuis {len(limited_urls)} site(s)...")
                website_descriptions = self._get_website_descriptions(limited_urls)
                print(f"‚úÖ {len(website_descriptions)} description(s) r√©cup√©r√©e(s)")
            
            # Charger les templates par d√©faut pour ce type d'offre
            default_templates = self._load_default_templates(offer_type)
            
            # Traiter les exemples de templates (fusionner avec les templates par d√©faut)
            processed_templates = []
            
            # D'abord ajouter les templates par d√©faut
            if default_templates:
                processed_templates.extend(default_templates)
                print(f"üìù Template par d√©faut charg√© pour type '{offer_type}'")
            
            # Ensuite ajouter les templates fournis par l'utilisateur (s'ils existent)
            if example_templates and isinstance(example_templates, list) and len(example_templates) > 0:
                print(f"üìù Traitement de {len(example_templates)} exemple(s) de template(s) utilisateur...")
                for template in example_templates:
                    # Si c'est une string, essayer de la parser en JSON, sinon garder comme texte
                    if isinstance(template, str):
                        try:
                            processed_templates.append(json.loads(template))
                        except json.JSONDecodeError:
                            processed_templates.append(template)
                    else:
                        processed_templates.append(template)
                print(f"‚úÖ {len(processed_templates)} template(s) au total (d√©faut + utilisateur)")
            elif default_templates:
                print(f"‚úÖ Utilisation du template par d√©faut uniquement")
            
            # Recherche de VRAIS vols avec Amadeus (via recherche intelligente)
            real_flights_data = None
            real_time_search_results = None  # TOUJOURS None - Tavily d√©sactiv√©
            search_metadata = {}
            
            # Priorit√© 1 : Si flight_input est fourni, utiliser la recherche intelligente
            # Priorit√© 2 : Sinon, logique classique (origine/destination)
            use_smart_search = bool(flight_input)
            
            print("=" * 80)
            print("üîç D√âBUT RECHERCHE DE VOLS - AMADEUS (SMART SEARCH)")
            print("=" * 80)
            print(f"   - Mode: {'SMART SEARCH (format libre)' if use_smart_search else 'CLASSIQUE (origine/destination)'}")
            print(f"   - flight_input fourni: {bool(flight_input)}")
            if flight_input:
                print(f"   - flight_input value: '{flight_input[:100]}'")
            print(f"   - offer_type: {offer_type}")
            print(f"   - travel_date: {travel_date}")
            print(f"   - return_date: {return_date}")
            print()
            
            # MODE 1 : Recherche intelligente (prioritaire si flight_input fourni)
            if use_smart_search:
                print(f"‚úàÔ∏è Mode SMART SEARCH - Recherche intelligente avec parsing automatique")
                print(f"   Input: '{flight_input}'")
                
                real_flights_data = self._search_flights_smart(flight_input, search_metadata)
                
                if real_flights_data:
                    print(f"‚úÖ {len(real_flights_data)} vol(s) trouv√©(s) via recherche intelligente")
                    print(f"   Strat√©gie utilis√©e: {search_metadata.get('search_strategy')}")
                    if search_metadata.get('parsed_data'):
                        parsed = search_metadata['parsed_data']
                        print(f"   Infos pars√©es:")
                        if parsed.get('origin_airport'):
                            print(f"      - Origine: {parsed['origin_airport']}")
                        if parsed.get('destination_airport'):
                            print(f"      - Destination: {parsed['destination_airport']}")
                        if parsed.get('departure_date'):
                            print(f"      - Date d√©part: {parsed['departure_date']}")
                        if parsed.get('return_date'):
                            print(f"      - Date retour: {parsed['return_date']}")
                else:
                    print(f"‚ö†Ô∏è Aucun vol trouv√© via recherche intelligente")
                    if search_metadata.get('parsed_data'):
                        print(f"   Mais infos pars√©es disponibles:")
                        parsed = search_metadata['parsed_data']
                        if parsed.get('origin_airport'):
                            print(f"      - Origine: {parsed['origin_airport']}")
                        if parsed.get('destination_airport'):
                            print(f"      - Destination: {parsed['destination_airport']}")
            
            # MODE 2 : Logique classique (fallback si pas de flight_input)
            elif travel_date or offer_type == "transport":
                print(f"‚úàÔ∏è Mode CLASSIQUE - Recherche par origine/destination (fallback)")
                print(f"   Note: Pour utiliser le format GDS/num√©ro de vol, remplissez flight_input")
                
                # Code simplifi√© : juste informer qu'on n'a pas de flight_input
                print(f"   ‚ÑπÔ∏è Aucun flight_input fourni")
                print(f"   üí° Pour une recherche automatique, utilisez le champ flight_input avec:")
                print(f"      - Format GDS: '18NOV-25NOV BRU JFK 10:00 14:00'")
                print(f"      - Num√©ro de vol: 'AF001 18/11/2025'")
                print(f"      - Texte libre: 'Vol AF001 de Paris √† NY le 18/11'")
                
                search_metadata['search_attempted'] = False
                search_metadata['failure_reason'] = ['no_flight_input_provided']
            
            else:
                print(f"   ‚ÑπÔ∏è Recherche de vols non activ√©e (pas de flight_input, pas de date)")
                search_metadata['search_attempted'] = False
            
            # V√©rification finale des r√©sultats
            print()
            print("üîç V√©rification r√©sultat recherche...")
            if not real_flights_data:
                if search_metadata.get('search_attempted', False):
                    print(f"‚ùå Aucun vol trouv√©")
                    print(f"   üí° V√©rifiez les logs ci-dessus pour plus de d√©tails")
                    if search_metadata.get('failure_reason'):
                        print(f"   üìã Raisons: {', '.join(search_metadata.get('failure_reason', []))}")
                else:
                    print(f"‚ÑπÔ∏è Recherche de vols non effectu√©e")
                
                search_metadata['has_valid_flight_info'] = False
                if 'source' not in search_metadata:
                    search_metadata['source'] = None
            else:
                print(f"‚úÖ {len(real_flights_data)} vol(s) trouv√©(s)")
                print(f"   Source: {search_metadata.get('source', 'unknown')}")
                if search_metadata.get('search_strategy'):
                    print(f"   Strat√©gie: {search_metadata['search_strategy']}")
            
            real_time_search_results = None  # Tavily toujours d√©sactiv√©
            
            print("=" * 80)
            print("FIN RECHERCHE DE VOLS")
            print("=" * 80)
            print()
            
            # Formater les vols r√©els pour le prompt ChatGPT
            real_flights_context = ""
            if real_flights_data:
                source_name = search_metadata.get('source', 'API')
                real_flights_context = f"\n\n‚úàÔ∏è‚úàÔ∏è‚úàÔ∏è VOLS R√âELS TROUV√âS (UTILISER CES DONN√âES - NE PAS INVENTER) :\n"
                real_flights_context += f"Source: {source_name}\n"
                
                for idx, flight in enumerate(real_flights_data, 1):
                    real_flights_context += f"\n--- Vol {idx} (R√âEL - DEPUIS {source_name.upper()}) ---\n"
                    real_flights_context += f"Num√©ro de vol: {flight.get('flight_number', 'N/A')}\n"
                    
                    # Compagnie (peut √™tre 'airline' ou 'carrier_code')
                    airline = flight.get('airline') or flight.get('carrier_code', 'N/A')
                    if airline != 'N/A':
                        real_flights_context += f"Compagnie: {airline}\n"
                    
                    # Infos vol
                    real_flights_context += f"D√©part: {flight.get('departure_airport', 'N/A')} √† {flight.get('departure_time', 'N/A')}\n"
                    real_flights_context += f"Arriv√©e: {flight.get('arrival_airport', 'N/A')} √† {flight.get('arrival_time', 'N/A')}\n"
                    
                    # Dur√©e si disponible
                    if flight.get('duration'):
                        real_flights_context += f"Dur√©e: {flight['duration']}\n"
                    
                    # Type de vol (direct/escales)
                    if 'stops' in flight:
                        if flight['stops'] == 0:
                            real_flights_context += f"Type: Vol direct\n"
                        else:
                            real_flights_context += f"Type: {flight['stops']} escale(s)\n"
                    
                    # Prix si disponible
                    if flight.get('price'):
                        real_flights_context += f"Prix: {flight['price']} {flight.get('currency', 'EUR')}\n"
                    
                    # Terminaux si disponibles
                    if flight.get('terminal_departure'):
                        real_flights_context += f"Terminal d√©part: {flight['terminal_departure']}\n"
                    if flight.get('terminal_arrival'):
                        real_flights_context += f"Terminal arriv√©e: {flight['terminal_arrival']}\n"
                
                real_flights_context += f"\nüö®üö®üö® CRITIQUE : Ces vols sont R√âELS et v√©rifiables. Utilise EXACTEMENT ces informations."
            
            # S√©lectionner le prompt selon le type d'offre
            # IMPORTANT: real_time_search_results est TOUJOURS None (Tavily d√©sactiv√©)
            # Seuls les vols r√©els d'Air France-KLM sont utilis√©s via real_flights_context
            print(f"üìù Pr√©paration des prompts:")
            print(f"   - real_time_search_results: {real_time_search_results} (Tavily d√©sactiv√©)")
            print(f"   - real_flights_context disponible: {bool(real_flights_context)}")
            if real_flights_context:
                print(f"   - Longueur real_flights_context: {len(real_flights_context)} caract√®res")
            
            if offer_type == "circuit":
                prompt = self._get_prompt_circuit(text_input, website_descriptions, processed_templates, travel_date, return_date, None, real_flights_context, offer_type)  # None pour Tavily
            elif offer_type == "sejour":
                prompt = self._get_prompt_sejour(text_input, website_descriptions, processed_templates, travel_date, return_date, None, real_flights_context, offer_type)  # None pour Tavily
            elif offer_type == "transport":
                prompt = self._get_prompt_transport(text_input, website_descriptions, processed_templates, None, travel_date, return_date, real_flights_context, offer_type)  # None pour Tavily
            else:
                # Par d√©faut, utiliser circuit
                prompt = self._get_prompt_circuit(text_input, website_descriptions, processed_templates, travel_date, return_date, None, real_flights_context, offer_type)  # None pour Tavily
            
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "Expert voyage. JSON uniquement."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=2500,
                temperature=0.7,
                timeout=90  # Timeout de 90 secondes pour √©viter les worker timeouts
            )
            
            offer_json = response.choices[0].message.content
            # Nettoyer le JSON
            if "```json" in offer_json:
                offer_json = offer_json.split("```json")[1].split("```")[0]
            elif "```" in offer_json:
                # Peut √™tre ``` sans json
                offer_json = offer_json.split("```")[1].split("```")[0]
            
            # Nettoyer le JSON avant parsing
            offer_json_clean = offer_json.strip()
            
            # Supprimer les caract√®res invisibles et nettoyer
            offer_json_clean = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', offer_json_clean)  # Supprimer caract√®res de contr√¥le
            
            # Supprimer les URLs d'images qui apparaissent comme texte dans le contenu
            # Pattern pour d√©tecter les URLs d'images (http/https avec extensions d'images)
            image_url_pattern = r'https?://[^\s"\'<>]+\.(jpg|jpeg|png|gif|webp|bmp|svg)(\?[^\s"\'<>]*)?'
            offer_json_clean = re.sub(image_url_pattern, '', offer_json_clean, flags=re.IGNORECASE)
            
            # Supprimer les r√©f√©rences "image url" ou "image:" qui pourraient rester
            offer_json_clean = re.sub(r'(?i)(image\s*url|image:\s*|url\s*image)[^\s]*', '', offer_json_clean)
            
            try:
                offer_structure = json.loads(offer_json_clean)
            except json.JSONDecodeError as e:
                print(f"‚ùå ERREUR PARSING JSON: {e}")
                print(f"üìÑ JSON RE√áU (premiers 1000 chars):\n{offer_json_clean[:1000]}")
                
                # Essayer de corriger les erreurs JSON courantes
                offer_json_fixed = offer_json_clean
                # Remplacer les apostrophes simples dans les cl√©s/valeurs par des doubles quotes
                offer_json_fixed = re.sub(r"'(\w+)':", r'"\1":', offer_json_fixed)  # Cl√©s avec apostrophes
                offer_json_fixed = re.sub(r':\s*\'([^\']*)\'', r': "\1"', offer_json_fixed)  # Valeurs avec apostrophes simples
                # Supprimer les virgules avant } ou ]
                offer_json_fixed = re.sub(r',(\s*[}\]])', r'\1', offer_json_fixed)
                # Corriger les guillemets non ferm√©s
                offer_json_fixed = re.sub(r'(\w+):\s*([^",{\[\s]+)(\s*[,\n}])', r'\1: "\2"\3', offer_json_fixed)
                
                try:
                    offer_structure = json.loads(offer_json_fixed)
                    print("‚úÖ JSON corrig√© avec succ√®s !")
                except json.JSONDecodeError as e2:
                    print(f"‚ùå √âchec correction JSON: {e2}")
                    # Essayer une derni√®re fois avec json5 ou avec une approche plus permissive
                    try:
                        # Extraire juste le JSON entre les premi√®res { et derni√®res }
                        start_idx = offer_json_fixed.find('{')
                        end_idx = offer_json_fixed.rfind('}')
                        if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
                            offer_json_fixed = offer_json_fixed[start_idx:end_idx+1]
                            offer_structure = json.loads(offer_json_fixed)
                            print("‚úÖ JSON extrait avec succ√®s !")
                        else:
                            raise e2
                    except:
                    # Si √ßa √©choue encore, retourner une erreur avec le JSON
                        error_context = offer_json_clean[max(0, e.pos-200):e.pos+200] if hasattr(e, 'pos') else offer_json_clean[:500]
                    return Response({
                        "error": f"Erreur parsing JSON OpenAI: {str(e)}",
                            "error_position": getattr(e, 'pos', None),
                            "raw_json_preview": error_context,
                            "hint": "Le JSON g√©n√©r√© par OpenAI contient des erreurs de format. Veuillez r√©essayer."
                    }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            
            # Pr√©parer les m√©tadonn√©es de tra√ßabilit√©
            airfrance_klm_used = search_metadata.get('source') == 'airfrance_klm'
            airfrance_klm_flights_count = search_metadata.get('real_flights_count', 0)
            
            print(f"üìä Pr√©paration m√©tadonn√©es:")
            print(f"   - search_metadata keys: {list(search_metadata.keys())}")
            print(f"   - search_metadata['source']: {search_metadata.get('source', 'NON D√âFINI')}")
            print(f"   - airfrance_klm_used: {airfrance_klm_used}")
            print(f"   - airfrance_klm_flights_count: {airfrance_klm_flights_count}")
            print(f"   - real_flights_data disponible: {bool(real_flights_data)}")
            if real_flights_data:
                print(f"   - Nombre de vols dans real_flights_data: {len(real_flights_data)}")
            print(f"   - travel_date fourni: {bool(travel_date)} ({travel_date if travel_date else 'NON FOURNI'})")
            print(f"   - return_date fourni: {bool(return_date)} ({return_date if return_date else 'NON FOURNI'})")
            
            metadata = {
                'search_info': {
                    'tavily_used': False,  # TOUJOURS False - Tavily d√©sactiv√©
                    'airfrance_klm_used': airfrance_klm_used,
                    'airfrance_klm_flights_count': airfrance_klm_flights_count,
                    'search_query': None,  # Pas de Tavily
                    'results_count': 0,  # Pas de Tavily
                    'travel_date': travel_date,
                    'return_date': return_date,
                    'is_round_trip': bool(return_date),
                    'destinations_detected': search_metadata.get('destinations', []),
                    'search_attempted': search_metadata.get('search_attempted', False),
                    'destination_code_found': search_metadata.get('destination_code_found', False),
                    'travel_date_provided': search_metadata.get('travel_date_provided', False),
                    'origin_code': search_metadata.get('origin_code'),
                    'destination_code': search_metadata.get('destination_code'),
                    'origin_provided': search_metadata.get('origin_provided'),
                    'destination_provided': search_metadata.get('destination_provided'),
                    'failure_reason': search_metadata.get('failure_reason', []),
                },
                'website_scraping': {
                    'urls_provided': website_urls if website_urls else [],
                    'urls_scraped': [],
                    'successful_scrapes': 0,
                    'failed_scrapes': 0
                },
                'templates_used': {
                    'default_template_loaded': bool(default_templates),
                    'user_templates_count': len(example_templates) if example_templates else 0,
                    'total_templates': len(processed_templates) if processed_templates else 0
                },
                'tavily_results': None,  # TOUJOURS None - Tavily d√©sactiv√©
                'airfrance_klm_flights': None  # Vols r√©els d'Air France-KLM si disponibles
            }
            
            # Ajouter les infos sur le scraping de sites
            if website_descriptions:
                metadata['website_scraping']['urls_scraped'] = [desc.get('url') for desc in website_descriptions if desc.get('url')]
                metadata['website_scraping']['successful_scrapes'] = len(website_descriptions)
            else:
                # Si des URLs ont √©t√© fournies mais aucune description r√©cup√©r√©e
                if website_urls and len(website_urls) > 0:
                    metadata['website_scraping']['failed_scrapes'] = len(website_urls)
                    metadata['website_scraping']['error'] = "Aucun site n'a pu √™tre scrapp√©. V√©rifiez que les URLs sont accessibles et que le site n'utilise pas de protection anti-scraping."
            
            # Ajouter les vols r√©els d'Air France-KLM (pas Tavily)
            if real_flights_data:
                metadata['airfrance_klm_flights'] = real_flights_data
                print(f"üìä M√©tadonn√©es: {len(real_flights_data)} vol(s) Air France-KLM ajout√©(s) aux m√©tadonn√©es")
            
            # Tavily est D√âSACTIV√â - plus de tavily_results
            print(f"üìä M√©tadonn√©es: tavily_results = None (Tavily d√©sactiv√©)")
            
            # Enrichir les sections avec des images (y compris celles des sites web)
            # Ajouter les images scrap√©es aux m√©tadonn√©es
            scraped_images = []
            if website_descriptions:
                for desc in website_descriptions:
                    if desc.get('images'):
                        scraped_images.extend(desc.get('images', []))
            
            # Passer les images scrap√©es √† la fonction d'enrichissement
            self.enrich_sections_with_images(offer_structure, scraped_images=scraped_images)
            
            # VALIDATION CRITIQUE POST-PARSING : Supprimer les sections de vol invent√©es si aucune source valide
            # IMPORTANT: Tavily est d√©sactiv√©, on utilise uniquement les vols Air France-KLM
            has_valid_flight_info = search_metadata.get('has_valid_flight_info', False)
            
            # Mettre √† jour has_valid_flight_info avec les vols r√©els d'Air France-KLM
            if real_flights_data:
                has_valid_flight_info = True
                print(f"‚úÖ‚úÖ‚úÖ Vols R√âELS d'Air France-KLM disponibles - validation pass√©e")
            
            # Si pas d'infos valides, SUPPRIMER toutes les sections de vol invent√©es
            if not has_valid_flight_info and offer_structure.get("sections"):
                print("‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è VALIDATION CRITIQUE : Aucune source valide trouv√©e pour les vols")
                print("üîç V√©rification et SUPPRESSION des sections de vol invent√©es...")
                
                sections = offer_structure.get("sections", [])
                flight_sections_to_remove = []
                sections_to_keep = []
                
                for section in sections:
                    section_type = (section.get("type", "") or "").lower()
                    section_title = (section.get("title", "") or "").lower()
                    section_id = (section.get("id", "") or "").lower()
                    
                    # D√©tecter les sections de vol
                    is_flight_section = (
                        section_type == "flights" or 
                        "vol" in section_title or 
                        "transport a√©rien" in section_title or
                        section_id == "flights"
                    )
                    
                    if is_flight_section:
                        flight_sections_to_remove.append(section.get("title", "Transport A√©rien"))
                        print(f"   ‚ùå Section SUPPRIM√âE (invent√©e) : '{section.get('title', 'Transport A√©rien')}'")
                        # NE PAS ajouter cette section √† sections_to_keep
                    else:
                        sections_to_keep.append(section)
                
                # Si on a supprim√© des sections de vol, ajouter une section d'avertissement
                if flight_sections_to_remove:
                    print(f"   ‚ö†Ô∏è {len(flight_sections_to_remove)} section(s) de vol SUPPRIM√âE(S)")
                    warning_section = {
                        "id": "flight_warning",
                        "type": "Avertissement",
                        "title": "‚ö†Ô∏è Informations de vol non disponibles",
                        "body": "Les informations de vol pour cette route/date ne sont pas disponibles dans nos sources consult√©es.\n\n‚ö†Ô∏è IMPORTANT : Les horaires, compagnies a√©riennes et num√©ros de vol doivent √™tre v√©rifi√©s directement aupr√®s des compagnies a√©riennes avant toute r√©servation.\n\nMerci de contacter directement les compagnies a√©riennes pour obtenir les informations de vol actuelles et v√©rifier les disponibilit√©s."
                    }
                    sections_to_keep.insert(0, warning_section)  # Ajouter au d√©but
                    offer_structure["sections"] = sections_to_keep
                    print(f"   ‚úÖ Section d'avertissement ajout√©e √† la place")
                    print("   ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è PROTECTION CLIENT : Les sections de vol invent√©es ont √©t√© SUPPRIM√âES automatiquement")
                else:
                    print("   ‚ÑπÔ∏è Aucune section de vol d√©tect√©e (pas besoin de suppression)")
            
            # Enrichir les sections de vol avec des preuves (liens et sources) - SEULEMENT si on a des sources valides
            if has_valid_flight_info:
                # Si on a des vols r√©els (Air France-KLM ou Aviationstack), les utiliser comme source
                if real_flights_data:
                    source_type = search_metadata.get('source', 'airfrance_klm')
                    if source_type == 'airfrance_klm':
                        print("‚úÖ‚úÖ‚úÖ Utilisation des vols R√âELS d'Air France-KLM comme source")
                        real_flights_source = {
                            "type": "Air France-KLM API (Vols r√©els v√©rifiables)",
                            "title": f"Vols r√©els Air France-KLM - {len(real_flights_data)} vol(s) trouv√©(s)",
                            "url": "https://developer.airfranceklm.com",
                            "description": f"Vols r√©els v√©rifiables depuis l'API Air France-KLM avec horaires, num√©ros de vol et compagnies a√©riennes confirm√©s"
                        }
                    else:
                        print("‚úÖ‚úÖ‚úÖ Utilisation des vols R√âELS d'Aviationstack comme source")
                        real_flights_source = {
                            "type": "Aviationstack (Vols r√©els v√©rifiables)",
                            "title": f"Vols r√©els {len(real_flights_data)} vol(s) trouv√©(s)",
                            "url": "https://aviationstack.com/",
                            "description": f"Vols r√©els v√©rifiables depuis l'API Aviationstack avec horaires, num√©ros de vol et compagnies a√©riennes confirm√©s"
                        }
                    # Ajouter les vols r√©els aux m√©tadonn√©es
                    search_metadata['real_flights'] = real_flights_data
                    self._enrich_flight_sections_with_sources(offer_structure, None, website_descriptions, search_metadata, real_flights_source=real_flights_source)
                else:
                    self._enrich_flight_sections_with_sources(offer_structure, None, website_descriptions, search_metadata, real_flights_source='airfrance_klm' if real_flights_data else None)
            else:
                print("‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è PROTECTION CLIENT : Aucune source valide - pas d'enrichissement des sections de vol (suppression effectu√©e)")
            
            # Nettoyer les URLs d'images du texte dans les sections
            if offer_structure.get("sections"):
                image_url_pattern = re.compile(
                    r'https?://[^\s"\'<>]+\.(jpg|jpeg|png|gif|webp|bmp|svg)(\?[^\s"\'<>]*)?',
                    re.IGNORECASE
                )
                
                # Pattern pour supprimer les r√©f√©rences textuelles aux images
                image_text_patterns = [
                    re.compile(r'(?i)(image\s*url|image:\s*|url\s*image|image\s*:\s*http)[^\s]*', re.IGNORECASE),
                    re.compile(r'(?i)(voir\s*l\'?image|voir\s*image|image\s*ci-dessous|image\s*ci-dessus)[^\n]*', re.IGNORECASE),
                ]
                
                for section in offer_structure["sections"]:
                    # Nettoyer le body
                    if section.get("body"):
                        body = str(section["body"])
                        # Supprimer les URLs d'images
                        body = image_url_pattern.sub('', body)
                        # Supprimer les r√©f√©rences textuelles aux images
                        for pattern in image_text_patterns:
                            body = pattern.sub('', body)
                        # Nettoyer les espaces multiples et lignes vides
                        body = re.sub(r'\n\s*\n\s*\n+', '\n\n', body)  # Max 2 sauts de ligne
                        body = re.sub(r' +', ' ', body)  # Un seul espace
                        section["body"] = body.strip()
                    
                    # Nettoyer le contenu
                    if section.get("content"):
                        content = str(section["content"])
                        content = image_url_pattern.sub('', content)
                        for pattern in image_text_patterns:
                            content = pattern.sub('', content)
                        content = re.sub(r'\n\s*\n\s*\n+', '\n\n', content)
                        content = re.sub(r' +', ' ', content)
                        section["content"] = content.strip()
                    
                    # Nettoyer la description (si elle existe)
                    if section.get("description"):
                        description = str(section.get("description"))
                        description = image_url_pattern.sub('', description)
                        for pattern in image_text_patterns:
                            description = pattern.sub('', description)
                        description = re.sub(r'\n\s*\n\s*\n+', '\n\n', description)
                        description = re.sub(r' +', ' ', description)
                        section["description"] = description.strip()
            
            # Nettoyer aussi l'introduction
            if offer_structure.get("introduction"):
                intro = str(offer_structure["introduction"])
                image_url_pattern = re.compile(
                    r'https?://[^\s"\'<>]+\.(jpg|jpeg|png|gif|webp|bmp|svg)(\?[^\s"\'<>]*)?',
                    re.IGNORECASE
                )
                intro = image_url_pattern.sub('', intro)
                intro = re.sub(r'(?i)(image\s*url|image:\s*|url\s*image)[^\s]*', '', intro)
                offer_structure["introduction"] = intro.strip()
            
            return Response({
                "offer_structure": offer_structure,
                "company_info": company_info,
                "metadata": metadata,  # ‚úÖ Ajouter les m√©tadonn√©es de tra√ßabilit√©
                "scraped_images": scraped_images[:10] if scraped_images else []  # ‚úÖ Ajouter les images scrap√©es
            })
            
        except Exception as e:
            return Response({"error": f"Erreur : {str(e)}"}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

    def pick_image_for_section(self, section):
        """S√©lectionne et cache une image pour une section donn√©e"""
        # Construire une query √† partir du titre/type
        title = section.get("title", "").replace("‚úàÔ∏è", "").replace("üöó", "").replace("üè®", "").replace("üéØ", "").replace("üí∞", "").strip()
        section_type = section.get("type", "")
        
        # Queries sp√©cifiques par type de section
        queries_map = {
            "Flights": f"{title} airplane travel",
            "Transfers": f"{title} airport transfer",
            "Hotel": f"{title} luxury hotel",
            "Activities": f"{title} travel activities",
            "Price": f"{title} travel booking"
        }
        
        query = queries_map.get(section_type, f"{title} {section_type}")
        
        images = []
        try:
            # Utiliser Unsplash par d√©faut (plus fiable) avec timeout r√©duit
            images = search_unsplash(query, per_page=1)
            if not images:
                # Fallback sur Bing si Unsplash √©choue
                images = search_bing_images(query, count=1)
        except Exception as e:
            print(f"Erreur recherche image pour {section_type}: {e}")
        
        # Cache l'image si trouv√©e (sans bloquer) - avec timeout court
        if images:
            try:
                original_url = images[0]["url"]
                # Utiliser l'URL originale directement pour √©viter le cache lent
                images[0]["url"] = original_url
                # Optionnel: cache en arri√®re-plan sans bloquer
                import threading
                def cache_in_background():
                    try:
                        cache_image(original_url)
                    except:
                        pass  # Ignore les erreurs de cache
                threading.Thread(target=cache_in_background, daemon=True).start()
            except Exception as e:
                print(f"Erreur cache image pour {section_type}: {e}")
                # Garde l'URL originale si le cache √©choue
        
        section["images"] = images or []
        return section

    def enrich_sections_with_images(self, offer_structure, scraped_images=None):
        """Enrichit chaque section avec des images appropri√©es.
        Priorit√© aux images scrap√©es depuis les sites web.
        Pour les circuits: priorit√© aux sections Hotel/H√©bergement/Activit√©s."""
        sections = offer_structure.get("sections", [])
        
        # Si on a des images scrap√©es, les utiliser en priorit√©
        if scraped_images and len(scraped_images) > 0:
            print(f"üì∏ Utilisation de {len(scraped_images)} image(s) scrap√©e(s) depuis les sites web")
            
            # D√©doublonner les images pour √©viter les r√©p√©titions
            unique_scraped_images = list(dict.fromkeys(scraped_images))  # Garde l'ordre, supprime les doublons
            if len(scraped_images) > len(unique_scraped_images):
                print(f"üì∏ {len(scraped_images) - len(unique_scraped_images)} image(s) en doublon supprim√©e(s)")
            
            # üè® PRIORIT√â pour les sections d'h√©bergement/h√¥tel dans les circuits
            hotel_sections = []
            activity_sections = []
            itinerary_sections = []
            other_sections = []
            
            for section in sections:
                section_type = section.get("type", "").lower()
                section_title = section.get("title", "").lower()
                
                if "hotel" in section_type or "h√©bergement" in section_title or "h√¥tel" in section_title:
                    hotel_sections.append(section)
                elif "activit" in section_type or "activit" in section_title or "excursion" in section_title:
                    activity_sections.append(section)
                elif "itin√©raire" in section_title or "itinerary" in section_type or "programme" in section_title:
                    itinerary_sections.append(section)
                else:
                    other_sections.append(section)
            
            # Ordre de priorit√© pour l'attribution des images
            prioritized_sections = hotel_sections + activity_sections + itinerary_sections + other_sections
            
            print(f"   üè® {len(hotel_sections)} section(s) d'h√¥tel/h√©bergement (priorit√© haute)")
            print(f"   üéØ {len(activity_sections)} section(s) d'activit√©s (priorit√© moyenne)")
            print(f"   üó∫Ô∏è {len(itinerary_sections)} section(s) d'itin√©raire (priorit√© moyenne)")
            print(f"   üìã {len(other_sections)} autre(s) section(s)")
            
            image_index = 0
            used_images = set()  # Tracker les images d√©j√† utilis√©es pour √©viter les doublons
            
            for section in prioritized_sections:
                section_type = section.get("type", "").lower()
                section_title = section.get("title", "").lower()
                
                # D√©terminer le nombre d'images en fonction du type de section
                is_hotel_section = "hotel" in section_type or "h√©bergement" in section_title or "h√¥tel" in section_title
                is_activity_section = "activit" in section_type or "activit" in section_title or "excursion" in section_title
                is_itinerary_section = "itin√©raire" in section_title or "itinerary" in section_type or "programme" in section_title
                
                # Plus d'images pour les sections importantes
                target_images = 3 if is_hotel_section else 2 if (is_activity_section or is_itinerary_section) else 2
                
                # Pour TOUTES les sections, utiliser des images scrap√©es si disponibles
                if image_index < len(unique_scraped_images) and not section.get("image") and not section.get("images"):
                    section_images = []
                    # Prendre le nombre d'images cibl√©
                    while len(section_images) < target_images and image_index < len(unique_scraped_images):
                        image_url = unique_scraped_images[image_index]
                        if image_url not in used_images:
                            section_images.append({"url": image_url})
                            used_images.add(image_url)
                        image_index += 1
                    
                    # Si on n'a pas assez d'images scrap√©es, compl√©ter avec des images API si disponible
                    if len(section_images) < target_images and (UNSPLASH_KEY or BING_KEY):
                        try:
                            # Chercher des images suppl√©mentaires via API
                            title = section.get("title", "")
                            body = section.get("body", "")
                            
                            # Cr√©er une requ√™te intelligente bas√©e sur le contenu
                            if is_hotel_section:
                                query = f"luxury hotel room {title}"
                            elif is_activity_section:
                                query = f"{title} travel activity"
                            else:
                                query = f"{title} travel"
                            
                            api_images = []
                            if UNSPLASH_KEY:
                                api_images = search_unsplash(query, per_page=target_images - len(section_images))
                            if not api_images and BING_KEY:
                                api_images = search_bing_images(query, count=target_images - len(section_images))
                            
                            for api_img in api_images:
                                additional_url = api_img.get("url")
                                if additional_url and additional_url not in used_images:
                                    section_images.append({"url": additional_url})
                                    used_images.add(additional_url)
                                    if len(section_images) >= target_images:
                                        break
                        except Exception as e:
                            print(f"   ‚ö†Ô∏è Erreur recherche image API pour section '{section.get('title')}': {e}")
                    
                    if section_images:
                        section["images"] = section_images
                        section["image"] = section_images[0]["url"]  # Format simple aussi pour compatibilit√©
                        emoji = "üè®" if is_hotel_section else "üéØ" if is_activity_section else "üì∏"
                        print(f"   {emoji} {len(section_images)} image(s) ajout√©e(s) √† la section '{section.get('title')}'")
                elif not section.get("image"):
                    # Si pas d'image scrap√©e disponible, chercher via API
                    # (on continue ci-dessous avec le code existant)
                    pass
        
        # V√©rifier si les APIs d'images sont configur√©es pour les sections restantes
        if not UNSPLASH_KEY and not BING_KEY:
            if not scraped_images or len(scraped_images) == 0:
                print("‚ö†Ô∏è Aucune API d'images configur√©e et aucune image scrap√©e - sections sans images")
            return
        
        # Limiter le nombre de sections pour √©viter les timeouts (seulement pour les sections sans images scrap√©es)
        sections_to_process = [s for s in sections if not s.get("image")][:2]  # Max 2 sections seulement
        
        # Traitement asynchrone des images pour √©viter les timeouts
        import threading
        import time
        
        def process_section_async(section):
            try:
                self.pick_image_for_section(section)
            except Exception as e:
                print(f"Erreur traitement image section {section.get('type', 'unknown')}: {e}")
                section["images"] = []  # Fallback s√ªr
        
        # Lancer les traitements en parall√®le avec timeout
        threads = []
        for section in sections_to_process:
            thread = threading.Thread(target=process_section_async, args=(section,))
            thread.daemon = True
            thread.start()
            threads.append(thread)
        
        # Attendre maximum 10 secondes pour tous les threads
        start_time = time.time()
        for thread in threads:
            remaining_time = 10 - (time.time() - start_time)
            if remaining_time > 0:
                thread.join(timeout=remaining_time)
            else:
                break

    def _enrich_flight_sections_with_sources(self, offer_structure, real_time_search_results, website_descriptions, search_metadata, real_flights_source=None):
        """Enrichit les sections de vol avec une seule source (la meilleure) pour prouver les informations"""
        sections = offer_structure.get("sections", [])
        
        # Trouver les sections de vol (Flights)
        flight_sections = [s for s in sections if s.get("type", "").lower() == "flights" or "vol" in s.get("title", "").lower() or "transport a√©rien" in s.get("title", "").lower()]
        
        if not flight_sections:
            print("‚ÑπÔ∏è Aucune section de vol trouv√©e pour enrichissement avec sources")
            return
        
        print(f"üìã Enrichissement de {len(flight_sections)} section(s) de vol avec une source de preuve...")
        
        # Pr√©parer la meilleure source disponible (priorit√© : Aviationstack > Tavily > Sites web)
        best_source = None
        
        # Priorit√© 1 : Source Air France-KLM ou Aviationstack (vols R√âELS) - LA MEILLEURE SOURCE
        if real_flights_source:
            best_source = real_flights_source
            source_name = "Air France-KLM" if "Air France-KLM" in best_source.get('type', '') else "Aviationstack"
            print(f"   ‚úÖ‚úÖ‚úÖ Source {source_name} (vols R√âELS) s√©lectionn√©e: {best_source['title']}")
        
        # Priorit√© 2 : Source depuis Tavily (recherche en temps r√©el) - seulement si pas de vols r√©els
        if not best_source and real_time_search_results:
            print(f"üîó {len(real_time_search_results)} source(s) Tavily disponibles pour les vols")
            for result in real_time_search_results:
                url = result.get("url", "")
                content = result.get("content", "").lower() or result.get("raw_content", "").lower()
                
                # V√©rifier que le r√©sultat contient r√©ellement des infos de vol (pas juste "aucun vol trouv√©")
                if url and content:
                    # D√©tecter les messages n√©gatifs (pas de vol trouv√©, aucun r√©sultat, etc.)
                    negative_indicators = [
                        "aucun vol", "no flights", "no results", "aucun r√©sultat",
                        "pas de vol", "vols non disponibles", "not available",
                        "aucune disponibilit√©", "no availability", "not found"
                    ]
                    has_negative_indicator = any(indicator in content for indicator in negative_indicators)
                    
                    if has_negative_indicator:
                        print(f"   ‚ö†Ô∏è Source Tavily exclue (message n√©gatif d√©tect√©): {result.get('title', 'Source')}")
                        continue
                    
                    # V√©rifier qu'il y a des infos positives (num√©ro de vol, horaires, compagnie, etc.)
                    positive_indicators = [
                        "vol", "flight", "departure", "d√©part", "arrival", "arriv√©e",
                        "airline", "compagnie", "terminal", "horaires", "schedule"
                    ]
                    has_positive_indicator = any(indicator in content for indicator in positive_indicators)
                    
                    if has_positive_indicator:
                        best_source = {
                            "type": "Tavily (Recherche temps r√©el)",
                            "title": result.get("title", "Source Tavily"),
                            "url": url,
                            "description": "Recherche en temps r√©el pour horaires et prix des vols"
                        }
                        print(f"   ‚úÖ Source Tavily s√©lectionn√©e: {best_source['title']} - {best_source['url']}")
                        break
                    else:
                        print(f"   ‚ö†Ô∏è Source Tavily exclue (pas d'infos de vol valides): {result.get('title', 'Source')}")
                elif url:
                    # Si on a une URL mais pas de contenu, on peut quand m√™me l'utiliser
                    best_source = {
                        "type": "Tavily (Recherche temps r√©el)",
                        "title": result.get("title", "Source Tavily"),
                        "url": url,
                        "description": "Recherche en temps r√©el pour horaires et prix des vols"
                    }
                    print(f"   ‚úÖ Source Tavily s√©lectionn√©e (sans contenu): {best_source['title']} - {best_source['url']}")
                    break
        
        # Priorit√© 2 : Source depuis les sites web scrap√©s (si pas de Tavily)
        if not best_source and website_descriptions:
            print(f"üîó {len(website_descriptions)} site(s) web scrap√©(s) disponibles comme sources")
            for desc in website_descriptions:
                url = desc.get("url", "")
                if url:
                    # V√©rifier si le contenu contient des infos de vol
                    content = desc.get("content", "").lower()
                    if any(keyword in content for keyword in ["vol", "flight", "compagnie", "airline", "a√©roport", "d√©part", "arriv√©e"]):
                        best_source = {
                            "type": "Site web scrap√©",
                            "title": f"Site web: {url[:50]}...",
                            "url": url,
                            "description": "Informations de vol extraites depuis le site web"
                        }
                        print(f"   ‚úÖ Source Site web s√©lectionn√©e: {best_source['url']}")
                        break
        
        # Logs d√©taill√©s
        if best_source:
            print(f"üìä Source de preuve s√©lectionn√©e pour les vols:")
            print(f"   Type: {best_source['type']}")
            print(f"   Titre: {best_source['title']}")
            print(f"   URL: {best_source['url']}")
            print(f"   Description: {best_source['description']}")
        else:
            print("‚ö†Ô∏è Aucune source de preuve disponible pour les vols (pas de recherche Tavily ni de sites web avec infos de vol)")
        
        # Ajouter la source unique √† chaque section de vol (METADATA SEULEMENT - PAS DE TEXTE)
        for flight_section in flight_sections:
            section_title = flight_section.get("title", "Transport A√©rien")
            print(f"\n‚úàÔ∏è Enrichissement section '{section_title}' avec source (metadata seulement)...")
            
            # Ajouter un champ "source" (singulier) avec le lien - METADATA SEULEMENT
            if best_source:
                flight_section["source"] = best_source  # Champ singulier pour m√©tadonn√©es
                print(f"   ‚úÖ Source ajout√©e aux m√©tadonn√©es de la section '{section_title}' (pas de modification du texte)")
                
                # ‚ùå NE PLUS ajouter le lien dans le body - l'utilisateur ne veut pas de sources/r√©f√©rences dans le texte
                # Le body reste tel quel, sans ajout de source
            else:
                print(f"   ‚ö†Ô∏è Aucune source disponible pour la section '{section_title}'")
                flight_section["source"] = None
                
                # ‚ùå NE PLUS ajouter de note critique dans le body
                # Le body reste tel quel, m√™me si les informations sont invent√©es
                print(f"   ‚ÑπÔ∏è Pas de source mais pas de note critique ajout√©e (texte propre demand√© par l'utilisateur)")


class PDFOfferGenerator(APIView):
    """
    G√©n√®re un PDF √† partir d'un texte d'offre
    Cette classe est d√©pr√©ci√©e - utiliser TravelOfferGenerator + GrapesJSPDFGenerator
    """
    permission_classes = [AllowAny]
    
    def post(self, request):
        text_input = request.data.get("text")
        company_info = request.data.get("company_info", {})
        
        if not text_input:
            return Response({"error": "Texte requis"}, status=status.HTTP_400_BAD_REQUEST)
        
        try:
            # G√©n√©rer d'abord la structure de l'offre
            travel_generator = TravelOfferGenerator()
            offer_response = travel_generator.post(request)
            
            if offer_response.status_code != 200:
                return offer_response
            
            offer_data = offer_response.data
            offer_structure = offer_data.get("offer_structure", {})
            
            # G√©n√©rer un HTML simple √† partir de la structure
            html_content = self._generate_html_from_structure(offer_structure)
            css_content = ""
            
            # G√©n√©rer le PDF avec WeasyPrint
            pdf_generator = GrapesJSPDFGenerator()
            printable_html = pdf_generator.convert_grapesjs_to_printable_html(
                html_content, 
                css_content, 
                company_info
            )
            
            pdf_bytes = HTML(string=printable_html).write_pdf()
            
            response = HttpResponse(pdf_bytes, content_type='application/pdf')
            response['Content-Disposition'] = 'attachment; filename="offre_voyage.pdf"'
            return response
            
        except Exception as e:
            traceback.print_exc()
            return Response({"error": f"Erreur : {str(e)}"}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    
    def _generate_html_from_structure(self, offer_structure):
        """G√©n√®re un HTML simple √† partir de la structure d'offre"""
        html_parts = []
        
        # Titre
        title = offer_structure.get("title", "Offre de voyage")
        html_parts.append(f"<h1>{title}</h1>")
        
        # Introduction
        intro = offer_structure.get("introduction", "")
        if intro:
            html_parts.append(f"<p>{intro}</p>")
        
        # Sections
        sections = offer_structure.get("sections", [])
        for section in sections:
            section_title = section.get("title", "")
            section_body = section.get("body", "")
            
            if section_title:
                html_parts.append(f"<h2>{section_title}</h2>")
            if section_body:
                html_parts.append(f"<div>{section_body}</div>")
        
        # CTA
        cta = offer_structure.get("cta", {})
        if cta:
            cta_title = cta.get("title", "")
            cta_desc = cta.get("description", "")
            
            if cta_title or cta_desc:
                html_parts.append("<div class='cta-section'>")
                if cta_title:
                    html_parts.append(f"<h2>{cta_title}</h2>")
                if cta_desc:
                    html_parts.append(f"<p>{cta_desc}</p>")
                html_parts.append("</div>")
        
        return "\n".join(html_parts)


class PdfToGJSEndpoint(APIView):
    """
    POST multipart/form-data:
      - file: le PDF
      - company_info: JSON string {name, phone, email, address, website}
      - logo_data_url (optionnel): string data:image/png;base64,...
      - background_url (optionnel): URL
    Retourne:
      {
        "offer_structure": {...},     # JSON pour Grapes
        "assets": [{"name":"img1.png","data_url":"data:image/png;base64,..."}],
        "company_info": {...},
        "background_url": "...",
        "logo_data_url": "..."
      }
    """
    permission_classes = [AllowAny]  # Acc√®s libre temporaire

    def post(self, request):
        pdf = request.FILES.get("file")
        if not pdf:
            return Response({"error": "Aucun PDF fourni"}, status=400)

        company_info = request.data.get("company_info") or "{}"
        try:
            company_info = json.loads(company_info)
        except:
            company_info = {}

        logo_data_url = request.data.get("logo_data_url") or ""
        background_url = request.data.get("background_url") or ""

        try:
            text_md, assets = self._extract_pdf_content(pdf)
            offer_structure = self._md_to_offer_json(text_md, company_info, assets)
            
            # V√©rifier que le contenu est valide
            if not offer_structure or not offer_structure.get('sections') or len(offer_structure.get('sections', [])) == 0:
                raise Exception("Aucun contenu structur√© n'a pu √™tre extrait du PDF")

            # Sauvegarder automatiquement le document import√©
            try:
                document = Document.objects.create(
                    title=offer_structure.get('title', 'PDF Import√©'),
                    description=f"Document import√© le {datetime.now().strftime('%d/%m/%Y √† %H:%M')}",
                    document_type='pdf_import',
                    offer_structure=offer_structure,
                    company_info=company_info,
                    assets=assets
                )
                
                # Sauvegarder le fichier PDF original
                pdf.seek(0)  # Reset file pointer
                pdf_file_content = ContentFile(pdf.read(), name=f"imported_{document.id}.pdf")
                document.pdf_file.save(f"imported_{document.id}.pdf", pdf_file_content)
                
                print(f"‚úÖ Document automatiquement sauvegard√© avec l'ID: {document.id}")
                
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur sauvegarde automatique: {e}")
                # Ne pas faire √©chouer l'import si la sauvegarde √©choue

            return Response({
                "offer_structure": offer_structure,
                "assets": assets,
                "company_info": company_info,
                "background_url": background_url,
                "logo_data_url": logo_data_url,
                "document_id": document.id if 'document' in locals() else None
            })
        except Exception as e:
            import traceback; traceback.print_exc()
            return Response({"error": f"Erreur import PDF: {e}"}, status=500)

    def _extract_pdf_content(self, pdf_file):
        """Retourne (markdown_consolide, assets_base64[])"""
        doc = fitz.open(stream=pdf_file.read(), filetype="pdf")
        chunks = []
        assets = []
        img_count = 0

        for page_num, page in enumerate(doc):
            # Texte en blocs (unifi√©)
            # Essayer markdown d'abord, sinon fallback sur text
            try:
                text = page.get_text("markdown")
            except (AssertionError, ValueError):
                # Si markdown n'est pas support√©, utiliser text
                try:
                    text = page.get_text("text")
                except Exception:
                    text = ""
            
            if text and text.strip():
                chunks.append(text.strip())

            # Images avec am√©liorations
            for img_index, img in enumerate(page.get_images(full=True)):
                pix = None
                try:
                    xref = img[0]
                    pix = fitz.Pixmap(doc, xref)
                    
                    # V√©rifier la taille de l'image (filtrer les trop petites)
                    if pix.width < 50 or pix.height < 50:
                        if pix:
                            pix = None
                        continue
                    
                    # Conversion CMYK ‚Üí RGB si n√©cessaire avec gestion d'erreur
                    try:
                        if pix.n > 4:  # CMYK ‚Üí RGB
                            old_pix = pix
                            pix = fitz.Pixmap(fitz.csRGB, pix)
                            old_pix = None  # Lib√©rer l'ancienne pixmap
                    except Exception as conv_error:
                        print(f"‚ö†Ô∏è Erreur conversion couleur image page {page_num+1}: {conv_error}")
                        # Essayer de continuer avec l'image originale
                        pass
                    
                    # Convertir en PNG avec gestion d'erreur
                    try:
                        img_bytes = pix.tobytes("png")
                    except Exception as png_error:
                        print(f"‚ö†Ô∏è Erreur conversion PNG page {page_num+1}: {png_error}")
                        if pix:
                            pix = None
                        continue
                    
                    # V√©rifier que l'image n'est pas trop grande (limite √† 1MB pour √©conomie RAM)
                    if len(img_bytes) > 1024 * 1024:  # 1MB max par image
                        print(f"‚ö†Ô∏è Image trop grande ({len(img_bytes)/1024:.0f}KB) - ignor√©e")
                        if pix:
                            pix = None
                        continue
                    
                    b64 = base64.b64encode(img_bytes).decode("ascii")
                    
                    # Ajouter des m√©tadonn√©es utiles
                    assets.append({
                        "name": f"pdf_image_page{page_num+1}_{img_index+1}.png",
                        "data_url": f"data:image/png;base64,{b64}",
                        "width": pix.width,
                        "height": pix.height,
                        "page": page_num + 1,
                        "size_kb": round(len(img_bytes) / 1024, 1)
                    })
                    img_count += 1
                    
                except Exception as e:
                    print(f"Erreur extraction image page {page_num+1}: {e}")
                    continue
                finally:
                    if pix:
                        pix = None

        doc.close()
        md = "\n\n".join(chunks)
        md = re.sub(r'\n{3,}', '\n\n', md).strip()
        return md, assets

    def _md_to_offer_json(self, markdown_text, company_info, assets=[]):
        """Demande √† l'IA de mapper le texte ‚Üí JSON sections normalis√©es."""
        # OPTIMISATION: Ne pas inclure les images dans le prompt OpenAI pour acc√©l√©rer le traitement
        # Les images seront ajout√©es automatiquement apr√®s la g√©n√©ration
        
        # Limiter la taille du texte pour √©viter les timeouts (max ~50000 caract√®res = ~12500 tokens)
        max_chars = 50000
        if len(markdown_text) > max_chars:
            print(f"‚ö†Ô∏è PDF tr√®s long ({len(markdown_text)} caract√®res), troncature √† {max_chars}")
            markdown_text = markdown_text[:max_chars] + "\n\n[... PDF tronqu√©, contenu trop long ...]"
        
        sys = """Expert en structuration d'offres de voyage. R√©ponds en JSON strict.
R√àGLE: Conserve 100% du texte original. Ne r√©sume JAMAIS, structure uniquement."""
        
        user = f"""
Structure cette offre en JSON avec: title, introduction, sections[], cta.
Sections possibles: Flights, Hotel, Price, Programme, Activities, Transfers, Info.
Format section: {{"id":"slug","type":"...","title":"...","body":"..."}}

CRITIQUE: Conserve TOUT le texte (tous les jours, d√©tails, listes). Reformate en markdown propre.

Contenu:
{markdown_text}
        """
        try:
            res = client.chat.completions.create(
                model="gpt-4o-mini",
                temperature=0.1,  # Plus bas pour plus de fid√©lit√© au texte original
                timeout=110,  # Timeout de 110 secondes (marge avant worker timeout √† 120s)
                max_tokens=10000,  # Augment√© pour PDFs complexes
                # gpt-4o-mini est tr√®s √©conomique: ~$0.15/$0.60 par 1M tokens (entr√©e/sortie)
                messages=[
                    {"role": "system", "content": sys},
                    {"role": "user", "content": user}
                ]
            )
        except Exception as e:
            print(f"‚ùå Erreur OpenAI API: {e}")
            # Ne PAS retourner de structure - lever l'exception pour √©viter de sauvegarder un document vide
            raise Exception(f"√âchec du traitement OpenAI: {str(e)}")
        raw = res.choices[0].message.content.strip()
        if raw.startswith("```"):
            raw = raw.split("```json")[-1].split("```")[0]
        data = json.loads(raw)

        # s√©curit√©: cl√©s minimales
        data.setdefault("title", "Votre offre de voyage")
        data.setdefault("introduction", "")
        data.setdefault("sections", [])
        data.setdefault("cta", {"title":"R√©servez maintenant","description":"","buttonText":"R√©server"})

        # Post-traitement: enrichir les sections avec les images extraites du PDF
        if assets and len(assets) > 0:
            print(f"üñºÔ∏è Enrichissement des sections avec {len(assets)} image(s) extraite(s) du PDF...")
            # Cr√©er un mapping des images disponibles
            images_by_type = {
                'flights': [],  # Images d'avions/a√©roports
                'hotel': [],    # Images d'h√¥tels/chambres/piscines
                'activities': [], # Images d'activit√©s/paysages
                'other': []     # Autres images
            }
            
            # Classifier les images selon leur nom/page (heuristique simple)
            for asset in assets:
                name_lower = asset['name'].lower()
                # Heuristique bas√©e sur le nom de l'image
                if any(keyword in name_lower for keyword in ['avion', 'airport', 'aeroport', 'flight', 'vol']):
                    images_by_type['flights'].append(asset)
                elif any(keyword in name_lower for keyword in ['hotel', 'chambre', 'room', 'piscine', 'pool', 'spa']):
                    images_by_type['hotel'].append(asset)
                elif any(keyword in name_lower for keyword in ['activite', 'activity', 'paysage', 'landscape', 'excursion']):
                    images_by_type['activities'].append(asset)
                else:
                    images_by_type['other'].append(asset)
            
            # Enrichir les sections avec les images appropri√©es
            for section in data.get("sections", []):
                section_type = (section.get("type") or "").lower()
                section_id = (section.get("id") or "").lower()
                
                # V√©rifier si la section a d√©j√† des images
                if "images" not in section or not section["images"]:
                    section["images"] = []
                
                # Associer les images selon le type de section
                images_to_add = []
                if section_type in ['flights', 'flight', 'transport'] or section_id in ['flights', 'flight']:
                    images_to_add = images_by_type['flights'][:2]  # Max 2 images par section
                elif section_type in ['hotel', 'hebergement'] or section_id in ['hotel', 'hebergement']:
                    images_to_add = images_by_type['hotel'][:2]
                elif section_type in ['activities', 'activities', 'programme', 'itinerary', 'itineraire'] or section_id in ['activities', 'activities', 'programme', 'itinerary']:
                    images_to_add = images_by_type['activities'][:2]
                
                # Si aucune image sp√©cifique trouv√©e, utiliser les images "other"
                if not images_to_add:
                    images_to_add = images_by_type['other'][:2]
                
                # Ajouter les images √† la section (format pour Puck editor)
                for asset in images_to_add:
                    # V√©rifier que l'image n'est pas d√©j√† dans la section
                    if not any(img.get("url") == asset["data_url"] for img in section["images"]):
                        section["images"].append({
                            "url": asset["data_url"],
                            "alt": asset.get("name", "Image du PDF"),
                            "caption": f"Image extraite de la page {asset.get('page', '?')} du PDF"
                        })
            
            # Si certaines images n'ont pas √©t√© associ√©es, les placer dans une section g√©n√©rale
            all_used_images = []
            for section in data.get("sections", []):
                for img in section.get("images", []):
                    if img.get("url"):
                        all_used_images.append(img["url"])
            
            unused_images = [asset for asset in assets if asset["data_url"] not in all_used_images]
            if unused_images:
                print(f"  ‚ö†Ô∏è {len(unused_images)} image(s) non associ√©e(s), ajout dans les sections...")
                # Ajouter dans les sections qui n'ont pas d√©j√† trop d'images
                remaining_unused = list(unused_images)
                for section in data.get("sections", []):
                    if len(section.get("images", [])) < 3 and remaining_unused:
                        for asset in remaining_unused[:3]:
                            section.setdefault("images", []).append({
                                "url": asset["data_url"],
                                "alt": asset.get("name", "Image du PDF"),
                                "caption": f"Image extraite de la page {asset.get('page', '?')} du PDF"
                            })
                        remaining_unused = remaining_unused[3:]
                    if not remaining_unused:
                        break
            
            print(f"‚úÖ Enrichissement termin√© - images int√©gr√©es dans les sections")

        return data


class ImproveOfferEndpoint(APIView):
    """
    POST JSON:
      { "offer_structure": {...}, "mode": "premium|concis|vendeur|familial|luxe" }
    Retourne: { "offer_structure": {...} } (m√™me sch√©ma)
    """
    permission_classes = [AllowAny]  # Acc√®s libre temporaire
    def post(self, request):
        data = request.data
        offer = data.get("offer_structure")
        mode = data.get("mode", "premium")

        if not offer:
            return Response({"error":"offer_structure manquant"}, status=400)

        sys = "Tu am√©liores la r√©daction d'une offre de voyage. Tu renvoies le m√™me JSON, champs identiques."
        tone_map = {
            "premium": "ton premium, clair et rassurant, orient√© valeur.",
            "concis": "ton concis et factuel.",
            "vendeur": "ton commercial soft, orient√© b√©n√©fices.",
            "familial": "ton chaleureux, accessible et rassurant.",
            "luxe": "ton haut de gamme, raffin√©, exclusif."
        }
        tone = tone_map.get(mode, tone_map["premium"])

        prompt = f"""Am√©liore l'offre ci-dessous (garde le m√™me JSON, m√™mes cl√©s),
r√©√©cris uniquement les champs textuels (title, introduction, sections[*].title/body, cta.*) avec un {tone}

JSON:
{json.dumps(offer, ensure_ascii=False)}
"""

        res = client.chat.completions.create(
            model="gpt-4o-mini",
            temperature=0.5,
            timeout=90,  # Timeout de 90 secondes pour √©viter les worker timeouts
            messages=[{"role":"system","content":sys},{"role":"user","content":prompt}]
        )
        raw = res.choices[0].message.content.strip()
        if raw.startswith("```"):
            raw = raw.split("```json")[-1].split("```")[0]
        improved = json.loads(raw)

        # ImproveOfferEndpoint ne g√©n√®re pas d'offre, donc pas de metadata
        return Response({
            "offer_structure": improved
        })


class DocumentListCreateView(ListCreateAPIView):
    """
    GET: Liste tous les documents sauvegard√©s
    POST: Cr√©e un nouveau document
    """
    queryset = Document.objects.all()
    
    def get(self, request):
        """Retourne la liste des documents avec leurs m√©tadonn√©es"""
        documents = Document.objects.all().order_by('-updated_at')
        data = []
        
        for doc in documents:
            data.append({
                'id': doc.id,
                'title': doc.title,
                'description': doc.description,
                'document_type': doc.document_type,
                'document_type_display': doc.get_document_type_display(),
                'created_at': doc.created_at.isoformat(),
                'updated_at': doc.updated_at.isoformat(),
                'file_size_mb': doc.file_size_mb,
                'has_pdf': bool(doc.pdf_file),
                'has_thumbnail': bool(doc.thumbnail),
                'company_info': doc.company_info,
                'assets_count': len(doc.assets) if doc.assets else 0
            })
        
        return Response(data)
    
    def post(self, request):
        """Sauvegarde un nouveau document"""
        try:
            title = request.data.get('title', 'Document sans titre')
            description = request.data.get('description', '')
            document_type = request.data.get('document_type', 'grapesjs_project')
            
            # Donn√©es GrapesJS
            grapes_html = request.data.get('grapes_html', '')
            grapes_css = request.data.get('grapes_css', '')
            offer_structure = request.data.get('offer_structure')
            company_info = request.data.get('company_info', {})
            assets = request.data.get('assets', [])
            
            # Cr√©er le document
            document = Document.objects.create(
                title=title,
                description=description,
                document_type=document_type,
                grapes_html=grapes_html,
                grapes_css=grapes_css,
                offer_structure=offer_structure,
                company_info=company_info,
                assets=assets
            )
            
            # Sauvegarder le PDF si fourni
            pdf_content = request.data.get('pdf_content')
            if pdf_content:
                # D√©coder le contenu base64 si n√©cessaire
                if pdf_content.startswith('data:application/pdf;base64,'):
                    pdf_content = pdf_content.split(',')[1]
                
                pdf_bytes = base64.b64decode(pdf_content)
                pdf_file = ContentFile(pdf_bytes, name=f"{document.id}_generated.pdf")
                document.pdf_file.save(f"{document.id}_generated.pdf", pdf_file)
            
            # Traiter les assets (images)
            if assets:
                for i, asset in enumerate(assets):
                    if isinstance(asset, dict) and 'data_url' in asset:
                        try:
                            # Extraire le contenu base64
                            header, data = asset['data_url'].split(',', 1)
                            file_bytes = base64.b64decode(data)
                            
                            # D√©terminer l'extension
                            if 'image/png' in header:
                                ext = 'png'
                            elif 'image/jpeg' in header:
                                ext = 'jpg'
                            else:
                                ext = 'png'
                            
                            # Cr√©er l'asset
                            asset_file = ContentFile(file_bytes, name=f"asset_{document.id}_{i}.{ext}")
                            
                            DocumentAsset.objects.create(
                                document=document,
                                name=asset.get('name', f'asset_{i}.{ext}'),
                                file=asset_file,
                                file_type=f'image/{ext}',
                                width=asset.get('width'),
                                height=asset.get('height'),
                                size_kb=asset.get('size_kb', len(file_bytes) / 1024)
                            )
                        except Exception as e:
                            print(f"Erreur sauvegarde asset {i}: {e}")
            
            return Response({
                'id': document.id,
                'title': document.title,
                'message': 'Document sauvegard√© avec succ√®s'
            }, status=status.HTTP_201_CREATED)
            
        except Exception as e:
            traceback.print_exc()
            return Response({'error': f'Erreur sauvegarde: {str(e)}'}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class DocumentDetailView(RetrieveUpdateDestroyAPIView):
    """
    GET: R√©cup√®re un document sp√©cifique avec tous ses d√©tails
    PUT/PATCH: Met √† jour un document
    DELETE: Supprime un document
    """
    queryset = Document.objects.all()
    
    def get(self, request, pk):
        """R√©cup√®re un document avec tous ses d√©tails"""
        try:
            document = Document.objects.get(pk=pk)
            
            # R√©cup√©rer les assets
            assets_data = []
            for asset in document.document_assets.all():
                assets_data.append({
                    'id': asset.id,
                    'name': asset.name,
                    'file_url': asset.file.url if asset.file else None,
                    'file_type': asset.file_type,
                    'width': asset.width,
                    'height': asset.height,
                    'size_kb': asset.size_kb
                })
            
            data = {
                'id': document.id,
                'title': document.title,
                'description': document.description,
                'document_type': document.document_type,
                'grapes_html': document.grapes_html,
                'grapes_css': document.grapes_css,
                'offer_structure': document.offer_structure,
                'company_info': document.company_info,
                'assets': document.assets,  # Assets originaux (base64)
                'stored_assets': assets_data,  # Assets stock√©s sur disque
                'pdf_url': document.pdf_file.url if document.pdf_file else None,
                'thumbnail_url': document.thumbnail.url if document.thumbnail else None,
                'created_at': document.created_at.isoformat(),
                'updated_at': document.updated_at.isoformat(),
                'file_size_mb': document.file_size_mb
            }
            
            return Response(data)
            
        except Document.DoesNotExist:
            return Response({'error': 'Document non trouv√©'}, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({'error': f'Erreur: {str(e)}'}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    
    def put(self, request, pk):
        """Met √† jour un document"""
        try:
            document = Document.objects.get(pk=pk)
            
            # Mettre √† jour les champs
            document.title = request.data.get('title', document.title)
            document.description = request.data.get('description', document.description)
            document.grapes_html = request.data.get('grapes_html', document.grapes_html)
            document.grapes_css = request.data.get('grapes_css', document.grapes_css)
            document.offer_structure = request.data.get('offer_structure', document.offer_structure)
            document.company_info = request.data.get('company_info', document.company_info)
            document.assets = request.data.get('assets', document.assets)
            
            document.save()
            
            return Response({
                'id': document.id,
                'title': document.title,
                'message': 'Document mis √† jour avec succ√®s'
            })
            
        except Document.DoesNotExist:
            return Response({'error': 'Document non trouv√©'}, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({'error': f'Erreur: {str(e)}'}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    
    def delete(self, request, pk):
        """Supprime un document"""
        try:
            document = Document.objects.get(pk=pk)
            title = document.title
            
            # Supprimer les fichiers associ√©s
            if document.pdf_file:
                document.pdf_file.delete()
            if document.thumbnail:
                document.thumbnail.delete()
            
            # Supprimer les assets
            for asset in document.document_assets.all():
                if asset.file:
                    asset.file.delete()
            
            document.delete()
            
            return Response({
                'message': f'Document "{title}" supprim√© avec succ√®s'
            })
            
        except Document.DoesNotExist:
            return Response({'error': 'Document non trouv√©'}, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({'error': f'Erreur: {str(e)}'}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class DocumentGeneratePDFView(APIView):
    """
    POST: G√©n√®re un PDF √† partir d'un document sauvegard√©
    """
    
    def post(self, request, pk):
        """G√©n√®re un PDF √† partir d'un document GrapesJS sauvegard√©"""
        try:
            document = Document.objects.get(pk=pk)
            
            if not document.grapes_html:
                return Response({'error': 'Pas de contenu HTML dans ce document'}, status=status.HTTP_400_BAD_REQUEST)
            
            # Utiliser la m√™me logique que GrapesJSPDFGenerator
            generator = GrapesJSPDFGenerator()
            printable_html = generator.convert_grapesjs_to_printable_html(
                document.grapes_html, 
                document.grapes_css, 
                document.company_info
            )
            
            with sync_playwright() as p:
                browser = p.chromium.launch()
                page = browser.new_page()
                page.set_content(printable_html, wait_until="load")
                pdf_bytes = page.pdf(
                    format="A4",
                    print_background=True,
                    margin={"top": "20mm", "bottom": "20mm", "left": "20mm", "right": "20mm"}
                )
                browser.close()
            
            # Sauvegarder le PDF g√©n√©r√© dans le document
            pdf_file = ContentFile(pdf_bytes, name=f"{document.id}_generated.pdf")
            document.pdf_file.save(f"{document.id}_generated.pdf", pdf_file)
            
            resp = HttpResponse(pdf_bytes, content_type="application/pdf")
            resp["Content-Disposition"] = f'inline; filename="{document.title}.pdf"'
            return resp
            
        except Document.DoesNotExist:
            return Response({'error': 'Document non trouv√©'}, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            traceback.print_exc()
            return Response({'error': f'Erreur g√©n√©ration PDF: {str(e)}'}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class FolderListCreateView(ListCreateAPIView):
    """
    GET: Liste tous les dossiers avec leur hi√©rarchie
    POST: Cr√©e un nouveau dossier
    """
    queryset = Folder.objects.all()
    
    def get(self, request):
        """Retourne la liste des dossiers avec leur hi√©rarchie"""
        # R√©cup√©rer tous les dossiers racines (sans parent)
        root_folders = Folder.objects.filter(parent=None).order_by('position', 'name')
        
        def serialize_folder(folder):
            return {
                'id': folder.id,
                'name': folder.name,
                'description': folder.description,
                'color': folder.color,
                'icon': folder.icon,
                'position': folder.position,
                'full_path': folder.full_path,
                'documents_count': folder.documents_count,
                'total_documents_count': folder.total_documents_count,
                'created_at': folder.created_at.isoformat(),
                'updated_at': folder.updated_at.isoformat(),
                'parent_id': folder.parent.id if folder.parent else None,
                'subfolders': [serialize_folder(sub) for sub in folder.subfolders.all()]
            }
        
        data = [serialize_folder(folder) for folder in root_folders]
        
        return Response(data)
    
    def post(self, request):
        """Cr√©e un nouveau dossier"""
        try:
            name = request.data.get('name', 'Nouveau dossier')
            description = request.data.get('description', '')
            color = request.data.get('color', '#3498DB')
            icon = request.data.get('icon', 'üìÅ')
            parent_id = request.data.get('parent_id')
            
            # V√©rifier que le parent existe si sp√©cifi√©
            parent = None
            if parent_id:
                try:
                    parent = Folder.objects.get(pk=parent_id)
                except Folder.DoesNotExist:
                    return Response({'error': 'Dossier parent non trouv√©'}, status=status.HTTP_404_NOT_FOUND)
            
            # Cr√©er le dossier
            folder = Folder.objects.create(
                name=name,
                description=description,
                color=color,
                icon=icon,
                parent=parent
            )
            
            return Response({
                'id': folder.id,
                'name': folder.name,
                'full_path': folder.full_path,
                'message': 'Dossier cr√©√© avec succ√®s'
            }, status=status.HTTP_201_CREATED)
            
        except Exception as e:
            traceback.print_exc()
            return Response({'error': f'Erreur cr√©ation dossier: {str(e)}'}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class FolderDetailView(RetrieveUpdateDestroyAPIView):
    """
    GET: R√©cup√®re un dossier avec ses documents
    PUT/PATCH: Met √† jour un dossier
    DELETE: Supprime un dossier
    """
    queryset = Folder.objects.all()
    
    def get(self, request, pk):
        """R√©cup√®re un dossier avec ses documents et sous-dossiers"""
        try:
            folder = Folder.objects.get(pk=pk)
            
            # R√©cup√©rer les documents du dossier
            documents = []
            for doc in folder.documents.all():
                documents.append({
                    'id': doc.id,
                    'title': doc.title,
                    'description': doc.description,
                    'document_type': doc.document_type,
                    'document_type_display': doc.get_document_type_display(),
                    'created_at': doc.created_at.isoformat(),
                    'updated_at': doc.updated_at.isoformat(),
                    'file_size_mb': doc.file_size_mb,
                    'has_pdf': bool(doc.pdf_file),
                    'has_thumbnail': bool(doc.thumbnail),
                    'assets_count': len(doc.assets) if doc.assets else 0
                })
            
            # R√©cup√©rer les sous-dossiers
            subfolders = []
            for sub in folder.subfolders.all():
                subfolders.append({
                    'id': sub.id,
                    'name': sub.name,
                    'description': sub.description,
                    'color': sub.color,
                    'icon': sub.icon,
                    'documents_count': sub.documents_count,
                    'total_documents_count': sub.total_documents_count
                })
            
            data = {
                'id': folder.id,
                'name': folder.name,
                'description': folder.description,
                'color': folder.color,
                'icon': folder.icon,
                'full_path': folder.full_path,
                'documents_count': folder.documents_count,
                'total_documents_count': folder.total_documents_count,
                'created_at': folder.created_at.isoformat(),
                'updated_at': folder.updated_at.isoformat(),
                'parent_id': folder.parent.id if folder.parent else None,
                'documents': documents,
                'subfolders': subfolders
            }
            
            return Response(data)
            
        except Folder.DoesNotExist:
            return Response({'error': 'Dossier non trouv√©'}, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({'error': f'Erreur: {str(e)}'}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    
    def put(self, request, pk):
        """Met √† jour un dossier"""
        try:
            folder = Folder.objects.get(pk=pk)
            
            # Mettre √† jour les champs
            folder.name = request.data.get('name', folder.name)
            folder.description = request.data.get('description', folder.description)
            folder.color = request.data.get('color', folder.color)
            folder.icon = request.data.get('icon', folder.icon)
            
            # Gestion du parent
            parent_id = request.data.get('parent_id')
            if parent_id:
                try:
                    parent = Folder.objects.get(pk=parent_id)
                    # V√©rifier qu'on ne cr√©e pas de boucle (simple v√©rification)
                    if parent.id == folder.id:
                        return Response({'error': 'Impossible de cr√©er une boucle dans la hi√©rarchie'}, 
                                      status=status.HTTP_400_BAD_REQUEST)
                    folder.parent = parent
                except Folder.DoesNotExist:
                    return Response({'error': 'Dossier parent non trouv√©'}, status=status.HTTP_404_NOT_FOUND)
            elif parent_id is None:
                folder.parent = None
            
            folder.save()
            
            return Response({
                'id': folder.id,
                'name': folder.name,
                'full_path': folder.full_path,
                'message': 'Dossier mis √† jour avec succ√®s'
            })
            
        except Folder.DoesNotExist:
            return Response({'error': 'Dossier non trouv√©'}, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({'error': f'Erreur: {str(e)}'}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    
    def delete(self, request, pk):
        """Supprime un dossier"""
        try:
            folder = Folder.objects.get(pk=pk)
            name = folder.name
            
            # V√©rifier s'il y a des documents ou sous-dossiers
            if folder.documents.exists() or folder.subfolders.exists():
                return Response({
                    'error': 'Impossible de supprimer un dossier non vide. D√©placez d\'abord son contenu.'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            folder.delete()
            
            return Response({
                'message': f'Dossier "{name}" supprim√© avec succ√®s'
            })
            
        except Folder.DoesNotExist:
            return Response({'error': 'Dossier non trouv√©'}, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({'error': f'Erreur: {str(e)}'}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class DocumentMoveToFolderView(APIView):
    """
    POST: D√©place un document vers un dossier
    """
    
    def post(self, request, document_id):
        """D√©place un document vers un dossier sp√©cifique"""
        try:
            document = Document.objects.get(pk=document_id)
            folder_id = request.data.get('folder_id')
            
            if folder_id:
                try:
                    folder = Folder.objects.get(pk=folder_id)
                    document.folder = folder
                except Folder.DoesNotExist:
                    return Response({'error': 'Dossier non trouv√©'}, status=status.HTTP_404_NOT_FOUND)
            else:
                # D√©placer vers la racine (aucun dossier)
                document.folder = None
            
            document.save()
            
            return Response({
                'message': f'Document "{document.title}" d√©plac√© avec succ√®s',
                'document_id': document.id,
                'folder_id': folder_id,
                'folder_name': document.folder.name if document.folder else 'Racine'
            })
            
        except Document.DoesNotExist:
            return Response({'error': 'Document non trouv√©'}, status=status.HTTP_404_NOT_FOUND)
        except Exception as e:
            return Response({'error': f'Erreur: {str(e)}'}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

class FreepikImageSearchView(APIView):
    """
    Proxy pour rechercher des images via l'API Freepik.
    R√©sout les probl√®mes CORS en passant par le backend.
    """
    permission_classes = [AllowAny]
    
    def get(self, request):
        """
        Recherche d'images Freepik
        
        Query params:
        - query: Terme de recherche (requis)
        - page: Num√©ro de page (d√©faut: 1)
        - limit: Nombre d'images par page (d√©faut: 30, max: 50)
        """
        try:
            query = request.GET.get('query', '').strip()
            page = int(request.GET.get('page', 1))
            limit = min(int(request.GET.get('limit', 30)), 50)  # Max 50
            
            if not query:
                return Response(
                    {'error': 'Le param√®tre "query" est requis'},
                    status=status.HTTP_400_BAD_REQUEST
                )
            
            # Cl√© API Freepik depuis settings
            api_key = getattr(settings, 'FREEPIK_API_KEY', 'FPSXdbb31bca84c6e7f77a957daa99cbd3b6')
            
            # Appel √† l'API Freepik avec filtres pour wallpapers/photos
            freepik_url = 'https://api.freepik.com/v1/resources'
            params = {
                'locale': 'en-US',
                'term': query,
                'page': page,
                'limit': limit,
                'order': 'relevance',  # Meilleure pertinence pour les recherches
                # Filtres pour avoir des vraies photos (pas flat backgrounds)
                'filters[content_type][photo]': 1,  # Seulement des photos
                'filters[orientation][landscape]': 1,  # Format paysage (wallpaper)
            }
            headers = {
                'Content-Type': 'application/json',
                'X-Freepik-API-Key': api_key
            }
            
            print(f"üîç Freepik API: Recherche '{query}' (page {page}, limit {limit})")
            print(f"   üì∏ Filtres: photos uniquement, format paysage (wallpaper)")
            
            response = requests.get(freepik_url, params=params, headers=headers, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                images_count = len(data.get('data', []))
                print(f"‚úÖ Freepik API: {images_count} images trouv√©es")
                
                return Response(data, status=status.HTTP_200_OK)
            else:
                error_data = response.json() if response.content else {}
                print(f"‚ùå Freepik API error {response.status_code}: {error_data}")
                
                return Response(
                    {
                        'error': f'Erreur Freepik API: {response.status_code}',
                        'details': error_data
                    },
                    status=response.status_code
                )
                
        except requests.Timeout:
            return Response(
                {'error': 'Timeout de l\'API Freepik'},
                status=status.HTTP_504_GATEWAY_TIMEOUT
            )
        except requests.RequestException as e:
            print(f"‚ùå Erreur r√©seau Freepik: {str(e)}")
            return Response(
                {'error': f'Erreur r√©seau: {str(e)}'},
                status=status.HTTP_502_BAD_GATEWAY
            )
        except Exception as e:
            print(f"‚ùå Erreur Freepik proxy: {str(e)}")
            traceback.print_exc()
            return Response(
                {'error': f'Erreur serveur: {str(e)}'},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
